[ { "title": "Update - Ansible Semaphore", "url": "/posts/ansible-semaphore-updates/", "categories": "Automation, Scripting", "tags": "servers, nginx, webserver, jekyll, automation, scripting, shell, bash, ansible, semaphore", "date": "2024-04-14 21:00:00 +0200", "snippet": "Integration von Ansible in Semaphore für Automatisierte Deployment-Prozesse Reboots mit Ansible und Semaphore: Du kannst Ansible in Semaphore nutzen, um Server neu zu starten und zu überprüfen...", "content": "Integration von Ansible in Semaphore für Automatisierte Deployment-Prozesse Reboots mit Ansible und Semaphore: Du kannst Ansible in Semaphore nutzen, um Server neu zu starten und zu überprüfen, ob diese Neustarts erfolgreich waren. Dies kann durch das ansible.builtin.reboot Modul erfolgen, das nicht nur die Maschinen neu startet, sondern auch sicherstellt, dass sie nach dem Neustart wieder ordnungsgemäß funktionieren. Verwendung von Datenbanken für Zustandsmanagement: Während Semaphore eine interne Datenbank für operationelle Zwecke nutzt, ist es empfehlenswert, für benutzerdefinierte Daten und Zustände eine separate Datenbank zu verwenden. Dies hilft, die Integrität der Semaphore-Datenbank zu bewahren und bietet dir mehr Flexibilität und Kontrolle über deine eigenen Daten. Sicheres Management von Konfigurationsdaten: Das Speichern von sensiblen Informationen wie Datenbankverbindungsdaten sollte sicher erfolgen. Wir haben verschiedene Methoden besprochen: Umgebungsvariablen: Eine gängige Methode zur Laufzeitkonfiguration, die das Speichern sensibler Daten außerhalb des Codes ermöglicht. Docker Secrets: Bietet eine sichere Speicherung und Verwaltung von Geheimnissen, ideal für den Einsatz in Docker Swarm Umgebungen. Externe Secrets Manager: Tools wie HashiCorp Vault oder AWS Secrets Manager bieten erweiterte Funktionen für das Secrets Management, insbesondere in komplexen Umgebungen. Erweiterte Nutzung von Ansible Playbooks: Zur Automatisierung und Überwachung von Server-Management-Aufgaben können Ansible Playbooks verwendet werden, um Zustände wie Festplattennutzung zu prüfen und Aktionen wie das Senden von Benachrichtigungen über Discord zu automatisieren.Praktische Anwendungsfälle Automatisierte Reboots: Nutze Ansible, um Server basierend auf bestimmten Kriterien (z.B. nach Updates) automatisch neu zu starten. Überwachung und Benachrichtigungen: Setze Playbooks ein, um Systemzustände zu überwachen und bei Bedarf Benachrichtigungen zu senden, etwa wenn der Festplattenspeicher knapp wird. Sicheres Config Management: Implementiere robuste Mechanismen für das Management von Konfigurationsdaten und Geheimnissen, um die Sicherheit und Compliance zu gewährleisten.Diese Strategien und Tools ermöglichen eine effiziente Verwaltung von IT-Infrastrukturen, automatisieren Routineaufgaben und erhöhen die Sicherheit durch sorgfältige Handhabung sensibler Daten. Nutze die Stärken von Ansible, Semaphore und Docker, um deine Deployment- und Operations-Prozesse zu optimieren.Automatisierung von Docker Compose mit Ansible Docker Compose und Ansible: Wir haben besprochen, wie man Ansible zusammen mit Docker Compose verwendet, um Dienste automatisch zu verwalten. Dies ist besonders nützlich, wenn regelmäßige Updates oder Neustarts von Services wie BIND9 erforderlich sind, die kritische Infrastrukturkomponenten darstellen. Ansible Playbook zur Serviceverwaltung: Du kannst ein Ansible Playbook verwenden, um Docker Compose Services zu steuern. Hierbei wird das community.docker.docker_compose_v2 Modul eingesetzt, um spezifische Dienste wie adblock und bind9 in einem Docker Compose-Projekt neu zu starten. Das Playbook ermöglicht es, den Zustand dieser Dienste zu kontrollieren und sicherzustellen, dass sie nach der Aktualisierung von Konfigurationen oder Daten korrekt laufen. Beispiel-Playbook:```yaml— name: Manage Docker Compose Services at /home/christian/bind9-dnshosts: allbecome: truetasks: name: Ensure Docker Compose application is runningcommunity.docker.docker_compose_v2: project_src: “/home/christian/bind9-dns” state: restarted services: - adblock - bind9register: output name: Send Discord message on successuri: url: “” method: POST body_format: json body: ‘{“content”: “Docker Compose services at /home/christian/bind9-dns have been successfully restarted on !”}’ headers: Content-Type: “application/json” status_code: 204when: output.changed``` Benachrichtigungen: Im Anschluss an die Neustartung der Services haben wir auch die Integration von Benachrichtigungen mittels Discord besprochen. Dies ermöglicht es, automatisierte Rückmeldungen über den Status der Neustartprozesse zu erhalten.Integration und Überwachung Monitoring und Feedback: Durch die Integration von Feedback-Mechanismen wie Discord-Benachrichtigungen kannst du den Zustand der Infrastruktur proaktiv überwachen und bei Bedarf schnell reagieren. Automatisierung von Wartungsprozessen: Die Kombination aus Ansible und Docker Compose bietet eine leistungsstarke Methode zur Automatisierung von Wartungs- und Update-Prozessen, die regelmäßige Interventionen erfordern, wie das Aktualisieren von DNS RPZ-Zonen in BIND9.Diese Techniken verbessern nicht nur die Effizienz der Systemverwaltung, sondern auch die Zuverlässigkeit und Reaktionsfähigkeit der IT-Infrastruktur. Indem du diese Prozesse automatisierst, minimierst du das Risiko von menschlichen Fehlern und stellst sicher, dass kritische Dienste stets aktuell und funktional sind." }, { "title": "Parken App Updates 0.0.6: Editiermodus, Zahlenschloss und Historienansicht", "url": "/posts/swift-parken-app-0-0-6/", "categories": "swuft, app", "tags": "swiftui, iOS, app, backend", "date": "2023-08-13 21:12:53 +0200", "snippet": "Parken App Updates 0.0.6: Editiermodus, Zahlenschloss und HistorienansichtHallo liebe Parken-App-Nutzer! Ich freue mich, euch in diesem Beitrag einige aufregende neue Funktionen in der neuesten Ver...", "content": "Parken App Updates 0.0.6: Editiermodus, Zahlenschloss und HistorienansichtHallo liebe Parken-App-Nutzer! Ich freue mich, euch in diesem Beitrag einige aufregende neue Funktionen in der neuesten Version 0.0.6 der Parken App vorstellen zu dürfen. Diese Verbesserungen machen die App nicht nur benutzerfreundlicher, sondern auch sicherer und informativer.EditiermodusEine der bemerkenswertesten Neuerungen ist der Editiermodus. Es ermöglicht dem Benutzer, bestimmte Fahrzeugdetails wie das Kennzeichen, die FIN/VIN oder den Status des Fahrzeugs zu ändern.Um den Editiermodus zu aktivieren, tippen Sie einfach auf einen der Einträge, und Sie werden in den Bearbeitungsmodus versetzt, in dem Sie die gewünschten Änderungen vornehmen können..onTapGesture(count: 1) {\tself.editMode = true}Ein langer Druck auf den Bildschirm speichert die Änderungen und beendet den Editiermodus:.onLongPressGesture {\tif self.editMode {\t\tsaveChanges()\t}}Zahlenschloss für den SchlüsselEine der interessantesten Neuerungen ist die Einführung eines Zahlenschlosses für den Schlüssel. Diese Funktion ermöglicht es dem Benutzer, über ein einfaches Picker-Interface eine vierstellige Nummer für den Schlüssel festzulegen:if title == \"Schlüssel Nr.:\" {\tHStack {\t\tdigitPicker(binding: $firstDigit)\t\tdigitPicker(binding: $secondDigit)\t\tdigitPicker(binding: $thirdDigit)\t\tdigitPicker(binding: $fourthDigit)\t}}HistorienansichtDie Parken App bietet jetzt eine Historienansicht, die eine Liste der verschiedenen Statusänderungen für das Fahrzeug anzeigt. Dies ist besonders nützlich, um einen Überblick über die verschiedenen Aktivitäten und Änderungen zu erhalten, die im Laufe der Zeit am Fahrzeug vorgenommen wurden.Ein weiteres Highlight dieser Funktion ist die Paginierung, die für eine nahtlose Benutzererfahrung bei der Durchsicht der Historie sorgt.VStack {\t// Griff\tRoundedRectangle(cornerRadius: 3)\t\t.fill(Color.gray)\t\t.frame(width: 50, height: 5)\t\t.padding(8)\tText(\"Historie\")\t\t.font(.headline)\t\t.padding(.bottom, 8)\tHistoryView(viewModel: HistoryViewModel(vehicleId: viewModel.vehicle.id))}AbschlussMit diesen neuen Funktionen hoffen wir, dass Ihr Fahrzeugmanagement mit der Parken App noch effizienter und angenehmer wird. Wir arbeiten ständig daran, die App zu verbessern und sind gespannt auf Ihr Feedback!" }, { "title": "Parken App Updates 0.0.5", "url": "/posts/swift-parken-app-0-0-5/", "categories": "swift, app", "tags": "swiftui, iOS, app, backend", "date": "2023-07-31 00:23:49 +0200", "snippet": "Liebes Tester Team,in dieser Version sind einige Bugs gefixt worden. App Abstürze aus dem Background heraus sind behoben worden.Folgende Neuerungen sind implementiert worden: parken: oben wird jet...", "content": "Liebes Tester Team,in dieser Version sind einige Bugs gefixt worden. App Abstürze aus dem Background heraus sind behoben worden.Folgende Neuerungen sind implementiert worden: parken: oben wird jetzt die aktive Gruppe angezeigt, der Kennzeichenscanner vergleicht den Anfang des Kennzeichens mit der Liste aller möglichen Zulassungsbezirke, das sollte die Genauigkeit erhöhen. Das Scannen allgemein geht jetzt wesentlich schneller. Kartenansicht: Die Karte ist jetzt beim ersten öffnen auf Deutschland gezoomt, bei jedem weiteren öffnen auf das zuletzt hinzugefügte Fahrzeug Fahrzeuge: In der Detailsansicht können jetzt Kennzeichen, FIN und Status angepasst werden, beim klicken auf speichern werden die Daten auf dem Server aktualisiert. Wenn das erfolgreich war werden die Daten auch auf dem Gerät aktualisiert. Profil: Gruppeneinladungen und Gruppe verlassen funktionieren jetzt wie vorgesehen.Viel Spaß beim Ausprobieren, ich freue mich auf euer Feedback" }, { "title": "Parken App und Backend - Updates und Bugfixes", "url": "/posts/swift-parken-app-0-0-4/", "categories": "swift, app", "tags": "pyhton, iOS, app, backend", "date": "2023-07-26 01:03:36 +0200", "snippet": "Wir freuen uns, Ihnen die neuesten Aktualisierungen und Bugfixes für unsere Parken App und das zugehörige Backend vorstellen zu können. Wir arbeiten kontinuierlich daran, die Benutzererfahrung zu v...", "content": "Wir freuen uns, Ihnen die neuesten Aktualisierungen und Bugfixes für unsere Parken App und das zugehörige Backend vorstellen zu können. Wir arbeiten kontinuierlich daran, die Benutzererfahrung zu verbessern und Probleme zu beheben, die sich auf die Leistung auswirken können. Hier ist, was in unserem jüngsten Update enthalten ist:Parken App1. Fahrzeugstatus Filter: Wir haben eine neue Funktion hinzugefügt, die es den Benutzern ermöglicht, nach dem Fahrzeugstatus zu filtern. Jetzt können Sie einfach nach Fahrzeugen suchen, die bestimmte Statuskriterien erfüllen.2. Suchfeld Verbesserungen: Die Suche ist jetzt nicht mehr case-sensitiv und unterstützt die Suche nach Kennzeichen und VIN (Vehicle Identification Number). Die Suchergebnisse aktualisieren sich jetzt in Echtzeit, während Sie tippen.3. Fehlerbehebung: Wir haben einen Absturz behoben, der auftrat, wenn auf einen Speicherbereich zugegriffen wurde, der nicht existiert. Dies wurde durch verbessertes Speicher- und Thread-Management behoben.4. Performance-Optimierungen: Um die App reaktionsschneller zu machen, haben wir die Performance verbessert, insbesondere beim Laden und Anzeigen der Fahrzeugliste.Backend1. Datenvalidierung: Wir haben unsere Datenvalidierung verbessert, um sicherzustellen, dass alle eingehenden Daten korrekt formatiert sind. Dies wird dazu beitragen, Fehler und Inkonsistenzen in der Datenbank zu reduzieren.2. API Optimierung: Die Performance unserer Backend-API wurde verbessert. Jetzt werden die Anfragen schneller bearbeitet, was zu einer verbesserten Benutzererfahrung führt.3. Datenbanksicherheit: Wir haben unsere Datenbanksicherheitsmaßnahmen aktualisiert, um die Daten unserer Benutzer noch besser zu schützen.4. Fehlerprotokollierung: Die Fehlerprotokollierung wurde verbessert, um uns zu helfen, Probleme schneller zu erkennen und zu beheben.Wir sind stolz auf die Verbesserungen, die wir in diesem Update vorgenommen haben, und wir freuen uns darauf, die Parken App und das Backend weiter zu verbessern, um unseren Benutzern den besten Service zu bieten. Wie immer, wenn Sie Feedback oder Vorschläge haben, würden wir uns freuen, von Ihnen zu hören." }, { "title": "Apple Push Notification Service mit Quart und Python: Ein Leitfaden", "url": "/posts/swift-apns/", "categories": "Programming, iOS", "tags": "swift, python, apns", "date": "2023-07-22 00:47:44 +0200", "snippet": "Echtzeitbenachrichtigungen sind ein wesentlicher Bestandteil moderner Anwendungen, um die Benutzererfahrung zu verbessern und die Benutzerbindung zu erhöhen. In diesem Blogbeitrag werde ich erkläre...", "content": "Echtzeitbenachrichtigungen sind ein wesentlicher Bestandteil moderner Anwendungen, um die Benutzererfahrung zu verbessern und die Benutzerbindung zu erhöhen. In diesem Blogbeitrag werde ich erklären, wie man den Apple Push Notification Service (APNs) mit Quart, einem asynchronen Python-Web-Framework, einrichtet und verwendet.VoraussetzungenBevor wir beginnen, stellen Sie sicher, dass Sie Folgendes haben: Ein APNs-Zertifikat, das von Apple ausgestellt wurde. Die Bundle-ID Ihrer App. Die Gerätekennung für jedes Gerät, das Benachrichtigungen empfangen soll.Quart-SetupZunächst einmal installieren wir Quart und aioapns, eine asynchrone APNs-Bibliothek für Python, indem wir den folgenden Befehl ausführen:pip install quart aioapnsJetzt, da wir Quart und aioapns installiert haben, können wir beginnen, unseren Code zu schreiben. Zunächst einmal erstellen wir eine neue Quart-App und definieren einige Konfigurationsvariablen.from quart import Quart, request, jsonify, make_responsefrom aioapns import APNs, Client, NotificationRequest, PushTypeapp = Quart(__name__)# Setzen Sie die Konfigurationsvariablenapp.config['APNS_KEY_FILE'] = '/path/to/your/key.pem' # Pfad zur Schlüsseldateiapp.config['APNS_CERT_FILE'] = '/path/to/your/cert.pem' # Pfad zur Zertifikatsdateiapp.config['APNS_TOPIC'] = 'com.yourcompany.yourapp' # Die Bundle-ID Ihrer AppEinrichten des APNs-ClientsNun, da wir unsere App konfiguriert haben, können wir den APNs-Client einrichten.@app.before_servingasync def setup_apns_client(): client = APNs( client_cert=app.config['APNS_CERT_FILE'], client_key=app.config['APNS_KEY_FILE'], use_sandbox=False, # Setzen Sie dies auf True, wenn Sie den Sandbox-APNs-Server verwenden ) app.config['APNS_CLIENT'] = clientAPI-Route zum Aktualisieren der Device-TokenWir werden eine API-Route erstellen, die es Clients erlaubt, ihre Device-Token zu aktualisieren.@app.route('/update-device-token', methods=['POST'])async def update_device_token(): data = await request.get_json() device_token = data.get('device_token') # Hier würden Sie normalerweise den Token in Ihrer Datenbank speichern. # Zum Zwecke dieses Tutorials drucken wir ihn einfach aus. print(f\"Received device token: {device_token}\") return make_response(jsonify({'message': 'Device token updated successfully'}), 200)Senden von Push-BenachrichtigungenJetzt können wir eine Funktion erstellen, um Push-Benachrichtigungen zu senden.async def send_push_notification(device_token, message): client = app.config['APNS_CLIENT'] notification_request = NotificationRequest( device_token=device_token, message={ \"aps\": { \"alert\": message, \"badge\": 1, } }, push_type=PushType.ALERT, ) try { response = await client.send_notification(notification_request) if response.is_successful: print(f\"Notification sent successfully to {device_token}\") else: print(f\"Failed to send notification: {response.error_reason}\") } except Exception as e { print(f\"An error occurred while sending the notification: {str(e)}\") }Jetzt können Sie die send_push_notification-Funktion verwenden, um eine Push-Benachrichtigung an ein bestimmtes Gerät zu senden. Geben Sie einfach den Gerätetoken und die Nachricht, die Sie senden möchten, als Argumente an.Und das war’s! Sie haben jetzt einen lauffähigen Quart-Server, der Push-Benachrichtigungen über den Apple Push Notification Service senden kann. Beachten Sie, dass dieser Code nur für Demonstration und Bildungszwecke gedacht ist und Sie ihn entsprechend Ihren Anforderungen anpassen und erweitern sollten, insbesondere im Hinblick auf Fehlerbehandlung und Sicherheit." }, { "title": "Künstliche Intelligenz und ihre Auswirkungen auf die Arbeitswelt: Eine Perspektive basierend auf LLM", "url": "/posts/ai-llm/", "categories": "AI", "tags": "KünstlicheIntelligenz,, Arbeitswelt,, LLM", "date": "2023-07-10 16:16:28 +0200", "snippet": "Künstliche Intelligenz (KI) durchläuft gerade eine beeindruckende Evolution, die unser Leben und unsere Arbeitswelt nachhaltig verändert. Ein entscheidendes Konzept in dieser Entwicklung ist das Le...", "content": "Künstliche Intelligenz (KI) durchläuft gerade eine beeindruckende Evolution, die unser Leben und unsere Arbeitswelt nachhaltig verändert. Ein entscheidendes Konzept in dieser Entwicklung ist das Lernen durch Latent Logic Modeling (LLM), ein fortschrittlicher Ansatz zum Erlernen komplexer Verhaltensweisen.LLM ist ein spezielles Gebiet des maschinellen Lernens, das darauf abzielt, Maschinen ein besseres Verständnis für die zugrunde liegende Logik von Daten zu vermitteln. Es basiert auf der Theorie, dass alle Verhaltensmuster - auch die komplexesten - in einer verborgenen, latenten Logik wurzeln. Mit anderen Worten, LLM versucht, die “Gedanken” hinter den Handlungen zu entschlüsseln.Wie wirkt sich das auf die Arbeitswelt aus? Künstliche Intelligenz wird zunehmend in vielen Bereichen eingesetzt, darunter Automatisierung, Datenanalyse, Entscheidungsfindung, Produktentwicklung und Kundenbetreuung. Die Integration von KI in diese Bereiche hat dazu geführt, dass Arbeitsprozesse effizienter, genauer und produktiver werden.Das LLM verbessert die KI-Fähigkeiten noch weiter. Durch das Verständnis der latenten Logik hinter Verhaltensmustern können KI-Systeme nicht nur wiederholbare Aufgaben ausführen, sondern auch adaptive und proaktive Maßnahmen ergreifen. Dies bedeutet, dass sie in der Lage sind, auf Veränderungen zu reagieren, Entscheidungen zu treffen und in gewissem Maße selbstständig zu agieren.Was bedeutet das für uns Menschen im Arbeitsumfeld? Zum einen können wir uns auf eine Steigerung der Produktivität freuen. KI-Systeme, die mit LLM ausgestattet sind, können repetitive und zeitaufwändige Aufgaben übernehmen, sodass wir uns auf komplexere und kreativere Aspekte unserer Arbeit konzentrieren können.Zum anderen stellt es uns vor neue Herausforderungen. Mit der zunehmenden Einführung von KI am Arbeitsplatz müssen wir lernen, mit KI-Systemen effektiv zusammenzuarbeiten und die richtigen Fähigkeiten entwickeln, um in einer zunehmend automatisierten Arbeitswelt wettbewerbsfähig zu bleiben.Ein weiterer Aspekt ist die ethische Dimension. Mit der zunehmenden Fähigkeit von KI-Systemen, Entscheidungen zu treffen, müssen wir klare Richtlinien und Regeln für die ethische Verwendung von KI festlegen. Dies umfasst Themen wie Verantwortlichkeit, Datenschutz und Fairness.Abschließend kann gesagt werden, dass Künstliche Intelligenz, insbesondere durch Fortschritte im LLM, das Potenzial hat, unsere Arbeitswelt tiefgreifend zu verändern. Es liegt an uns, diese Veränderungen positiv zu gestalten und eine Zukunft zu schaffen, in der KI und Mensch im Arbeitsumfeld harmonisch zusammenarbeiten.Image:DALL-E: Text für das Image kam von Chat GPT" }, { "title": "Wanderung um die Punta del Sorda: Ein Ausflug in das Herz von Teneriffa", "url": "/posts/punta-del-sorda/", "categories": "Reisen, Teneriffa", "tags": "teneriffa, wandern, natur, punta del sorda", "date": "2023-07-10 15:16:28 +0200", "snippet": "Eine der faszinierendsten Aspekte von Teneriffa ist die beeindruckende Vielfalt an Landschaften, die auf einer einzigen Insel zu finden sind. Von den schroffen Vulkanlandschaften des Teide-National...", "content": "Eine der faszinierendsten Aspekte von Teneriffa ist die beeindruckende Vielfalt an Landschaften, die auf einer einzigen Insel zu finden sind. Von den schroffen Vulkanlandschaften des Teide-Nationalparks bis hin zu den grünen Lorbeerwäldern des Anaga-Gebirges bietet Teneriffa eine Fülle von Erlebnissen für Outdoor-Enthusiasten. Eine meiner bemerkenswertesten Wanderungen führte mich kürzlich um die Punta del Sorda, einen verborgenen Schatz in der malerischen Landschaft von Teneriffa.Der Rundweg um die Punta del Sorda ist mit etwa sechs Kilometern ein relativ kurzer Wanderweg, der jedoch mit atemberaubenden Aussichten und einigen körperlichen Herausforderungen aufwarten kann. Angesichts der sengenden Hitze von 31 Grad Celsius, die uns an diesem Tag begleitete, war eine ausreichende Versorgung mit Wasser und Sonnenschutz unerlässlich.Der Weg schlängelte sich durch eine Vielfalt von Pflanzen und Blumen, darunter einige endemische Arten, die nur auf den Kanarischen Inseln zu finden sind. Es war beeindruckend zu sehen, wie das Leben selbst unter den trockensten und unwirtlichsten Bedingungen gedeiht.Der Pfad führte uns weiter zu einigen atemberaubenden Aussichtspunkten mit Blick auf das azurblaue Meer und die zerklüfteten Klippen der Küste. Die spektakulären Ausblicke auf den Atlantik, eingebettet in die raue Schönheit der umliegenden Landschaft, waren die perfekte Belohnung für die Anstrengungen des Aufstiegs.Obwohl die Hitze eine zusätzliche Herausforderung darstellte, trug sie auch zur Schönheit des Erlebnisses bei. Die warmen Farben des Felsens, die im intensiven Sonnenlicht leuchteten, und die glitzernde Meeresoberfläche unter dem strahlend blauen Himmel machten die Wanderung zu einem wahrhaft eindrucksvollen Erlebnis.Als wir schließlich die Rundwanderung beendet hatten, fühlten wir uns erfüllt und dankbar für die Möglichkeit, einen weiteren Aspekt der unglaublichen natürlichen Vielfalt Teneriffas erlebt zu haben. Trotz der Herausforderungen war die Wanderung um die Punta del Sorda ein unvergessliches Abenteuer und ein weiterer Beweis dafür, dass die Natur immer der beste Spielplatz ist.Egal ob du ein erfahrener Wanderer oder einfach nur ein Naturliebhaber bist, die Punta del Sorda ist definitiv einen Besuch wert. Aber denke daran, dich auf das Wetter vorzubereiten und genügend Wasser mitzunehmen. Denn wie diese Wanderung zeigt, kann Teneriffa ein heißes Pflaster sein!Für diejenigen, die sich auf den Weg machen möchten, können Sie die genaue Lage der Punta del Sorda hier auf Apple Maps sehen." }, { "title": "Blog Update: Automatisierung der Bildoptimierung und WebP-Konvertierung", "url": "/posts/blog-update/", "categories": "Blog, Update", "tags": "update, jekyll, bash, script, webp, jpegoptim, cwebp, base64", "date": "2023-07-09 23:47:49 +0200", "snippet": "Wir haben ein Bash-Skript entwickelt und eingerichtet, das inotify verwendet, um neu hinzugefügte Bilder zu überwachen. Sobald ein neues Bild erkannt wird, optimiert das Skript das Bild mit jpegopt...", "content": "Wir haben ein Bash-Skript entwickelt und eingerichtet, das inotify verwendet, um neu hinzugefügte Bilder zu überwachen. Sobald ein neues Bild erkannt wird, optimiert das Skript das Bild mit jpegoptim, konvertiert es dann mit cwebp in das WebP-Format und erzeugt eine LQIP-Version des Bildes für eine verbesserte Seitendarstellung während des Ladens. Erstellung von Markdown-Posts automatisieren: Das Skript generiert zudem automatisch eine Markdown-Datei für jeden neuen Blog-Post, die in Jekyll verwendet wird. Diese Datei enthält Header-Informationen sowie Links zu den optimierten Bildern. Implementierung von inotify für Dateilöschungen: Wir haben versucht, inotifywait so einzurichten, dass es auf gelöschte Dateien reagiert und einen rsync-Befehl ausführt, um Änderungen zwischen lokalen und entfernten Verzeichnissen zu synchronisieren. Allerdings sind wir auf einige Herausforderungen gestoßen und haben letztendlich entschieden, syncthing für die Synchronisation zu verwenden. Einrichten von Syncthing für Dateisynchronisation: Wir haben Syncthing auf dem Server installiert und konfiguriert, um Änderungen im Blog-Post-Verzeichnis in Echtzeit zu überwachen und zu synchronisieren. Dies hat den Vorteil, dass es bidirektional arbeitet und Dateiänderungen sowohl lokal als auch auf dem entfernten Server überwacht. Zusammengefasst haben wir heute einen signifikanten Automatisierungsprozess implementiert, der die Arbeit mit Bildern in Jekyll-Posts stark vereinfacht. Dies wird nicht nur die Qualität und Performance der Website verbessern, sondern auch viel Zeit sparen, die sonst für manuelles Optimieren, Konvertieren und Hochladen der Bilder aufgewendet werden müsste." }, { "title": "Bergabenteuer in Masca, Ein Ausflug nach Teneriffa", "url": "/posts/masca-ausflug/", "categories": "Reisen, Teneriffa", "tags": "teneriffa, wandern, natur, masca", "date": "2023-07-08 10:42:31 +0200", "snippet": "Ein Abenteuer inmitten von erstaunlichem Gelände, atemberaubenden Landschaften und einer Erfahrung, die einen bleibenden Eindruck hinterlässt, das ist es, was Masca auf Teneriffa zu bieten hat. Die...", "content": "Ein Abenteuer inmitten von erstaunlichem Gelände, atemberaubenden Landschaften und einer Erfahrung, die einen bleibenden Eindruck hinterlässt, das ist es, was Masca auf Teneriffa zu bieten hat. Diese verborgene Perle, die in den westlichen Ausläufern des Tenogebirges liegt, ist ein Muss für jeden, der das wahre Gesicht von Teneriffa entdecken möchte.Ich begann meinen Tag früh, bereit, die Herausforderungen zu meistern, die Masca bereithält. Die Fahrt nach Masca ist ein Abenteuer für sich. Die kurvenreiche Straße, die durch die Berge windet, bietet atemberaubende Ausblicke auf die grünen Schluchten und das tiefblaue Meer in der Ferne. Angekommen im Dorf Masca, war ich beeindruckt von der Schönheit der terrassenförmig angelegten Häuser, die an den steilen Berghängen klebten.Der Aufstieg war anspruchsvoll, aber jede Anstrengung wert. Der Wanderpfad schlängelte sich durch steile Felswände, entlang von alten Bewässerungskanälen und über steinige Treppenstufen, die in den Berg gehauen waren. Nach einem anspruchsvollen Aufstieg erreichte ich schließlich den höchsten Punkt auf 1280 Metern. Der Blick von dort oben war einfach atemberaubend. Es war ein Moment des Triumphs und der Bewunderung für die majestätische Natur, die mich umgab.Die Wanderung ging weiter durch den Lorbeerwald, vorbei an kleinen Bauernhöfen, bis ich schließlich zu dem Punkt kam, an dem der Blick auf die atemberaubende Landschaft von Masca freigegeben wurde. Es war wie eine andere Welt, mit den spitzen Felsformationen, die aus dem grünen Tal ragten und von tiefen Schluchten umgeben waren.Am Ende dieses unvergesslichen Tages, als ich auf die Erlebnisse zurückblickte, war ich erfüllt von einer tiefen Dankbarkeit. Nicht nur für die körperliche Herausforderung und die Erreichung des Gipfels, sondern auch für die Möglichkeit, eine so atemberaubende und unberührte Landschaft zu erleben. Masca ist ein Ort, der seine Besucher inspiriert und einen tiefen Eindruck hinterlässt.Der Ausflug nach Masca war mehr als nur ein Tag in der Natur. Es war eine Reise der Selbstentdeckung, des Abenteuers und der Bewunderung für die natürliche Schönheit, die uns umgibt. Und während ich den Tag mit schmerzenden Muskeln und einem Lächeln auf dem Gesicht beendete, wusste ich, dass ich eine Erfahrung gemacht hatte, die ich nie vergessen werde." }, { "title": "Scripting automate new jekyll-posts", "url": "/posts/scripting-automate-new-jekyll-posts/", "categories": "Programming, Scripting", "tags": "servers, nginx, webserver, jekyll, automation, scripting, shell, bash", "date": "2023-07-08 01:00:00 +0200", "snippet": "This script aims to automate the process of creating a new blog post in Jekyll, specifically optimizing images and generating their associated Base64 Low Quality Image Placeholders (LQIP).Here’s an...", "content": "This script aims to automate the process of creating a new blog post in Jekyll, specifically optimizing images and generating their associated Base64 Low Quality Image Placeholders (LQIP).Here’s an overview of the script and its functions:1. Preparation:Initially, the script defines necessary directories and ensures they exist. It creates them if they don’t exist.2. Directory Monitoring:It uses inotifywait to monitor the directory where new images are uploaded for new files. It responds to “create” and “moved_to” events and processes every new .jpg file added to the directory.3. Post Creation:For each uploaded image, a new blog post is created based on the image’s directory name. The script generates a Jekyll-compatible markdown file with metadata like title, date, categories, tags, and author. It also creates a special image metadata field that contains the path to the image file and the Base64 string of the LQIP.4. Image Optimization:Each image is copied and optimized to achieve a smaller file size. After that, the optimized image is converted to the WebP format and moved to a special directory accessible by the Nginx webserver.5. LQIP Generation:In addition, a low-resolution version of the image is created, converted to Base64, and stored in a file. This is used to generate the LQIPs for the blog post, which are displayed during the page load before the actual image is loaded.6. Cleanup:After all steps are successfully completed, the script deletes the temporary and optimized images.7. Synchronization:Finally, the script uses syncthing, to sync the generated Base64 files and blog post files to a remote server.All in all, this script serves to optimize and automate the process of creating blog posts. It reduces the effort associated with manual image optimization and LQIP creation and simplifies the creation of blog posts in Jekyll.Here is the full script. Remember that you will need to adjust the variables and paths to suit your own environment:#!/bin/bashdir=/path/to/importbase64dir=$dir/path/to/base64imagedir=/path/to/public/imagespostsdir=/path/to/postsremote_dir=user@jekyll-server:~/path/to/newpostsecho \"Script started.\" &gt; /var/log/webp-converter/info.logmkdir -p $base64dir $imagedir $postsdir || { echo \"Failed to create directories\"; exit 1; }image_count=0echo \"Image count is now $image_count\" &gt;&gt; /var/log/webp-converter/info.loginotifywait -m $dir -e create -e moved_to --format '%w%f' -r | while read file; do\techo \"New File detected: $file\" &gt;&gt; /var/log/webp-converter/info.log relative_path=${file#$dir/} if [[ $file =~ .jpg$ ]] &amp;&amp; [[ $file != *_optimized.jpg ]] &amp;&amp; [[ $file != .* ]]; then echo \"Processing $file...\" &gt;&gt; /var/log/webp-converter/info.log\t\t postname=$(basename $(dirname $file)) filename=$(basename $file .jpg) postdir=$imagedir/$postname mkdir -p $postdir || { echo \"Failed to create directory $postdir\"; continue; } mdfile=$postsdir/$(date +'%Y-%m-%d')-${postname}.md optimized=$dir/${relative_path%.*}_optimized.jpg rsync -avP $file $optimized || { echo \"Failed to copy $file\"; continue; } jpegoptim -s $optimized || { echo \"Failed to optimize $file\"; continue; } webp=$postdir/${filename}.webp cwebp -q 80 $optimized -o $webp || { echo \"Failed to convert $file to WebP\"; continue; } base64file=$base64dir/${filename}_optimized.base64 convert $optimized -resize 20 - | base64 | tr -d '\\n' &gt; $base64file || { echo \"Failed to convert $file to Base64\"; continue; } image_path=\"https://images.example.com/blog/$postname/${filename}.webp\" base64_string=$(cat $base64file) image_count=$(grep -c \"image:\" $mdfile)\t\techo \"Image count is now $image_count\" &gt;&gt; /var/log/webp-converter/info.log if [[ $image_count -eq 0 ]]; then\t\techo \"Writing header to $mdfile\" &gt;&gt; /var/log/webp-converter/info.log echo \"---\" &gt; $mdfile || { echo \"Failed to create $mdfile\"; continue; } echo \"layout: post\" &gt;&gt; $mdfile\t\techo \"title: \\\"Title Here\\\"\" &gt;&gt; $mdfile echo \"date: $(date +'%Y-%m-%d %H:%M:%S %z')\" &gt;&gt; $mdfile echo \"categories: category1 category2\" &gt;&gt; $mdfile echo \"tags: tag1 tag2\" &gt;&gt; $mdfile echo \"image:\" &gt;&gt; $mdfile || { echo \"Failed to write to $mdfile\"; continue; } echo \" path: $image_path\" &gt;&gt; $mdfile echo \" lqip: data:image/jpeg;base64,$base64_string\" &gt;&gt; $mdfile\t\t echo \"---\" &gt;&gt; $mdfile\t\t\t((image_count++)) echo \"Image count is now $image_count\" &gt;&gt; /var/log/webp-converter/info.log\t else\t\techo \"Writing image link to $mdfile\" &gt;&gt; /var/log/webp-converter/info.log echo \"![$filename!]($image_path){: w=\\\"800\\\" h=\\\"600\\\" lqip=\\\"data:image/jpeg;base64,$base64_string\\\" }\" &gt;&gt; $mdfile fi\t\techo \"Finished processing $file\" &gt;&gt; /var/log/webp-converter/info.log [ -e \"$optimized\" ] &amp;&amp; rm $optimized\t echo \"$file processed successfully\" fi doneecho \"Script Finished.\" &gt;&gt; /var/log/webp-converter/info.logWith this script, you can upload the images to the uploads/import directory and the script monitors this directory to automatically create a new blog post, optimize the image, and generate the LQIP. Please note that you will need the correct login credentials for your remote server and the jpegoptim, cwebp and convert (part of ImageMagick) commands must be installed on your server for the script to work." }, { "title": "Ein Besuch im Loro Parque", "url": "/posts/loro-parque/", "categories": "Reisen, Teneriffa", "tags": "urlaub, loro-parque, teneriffa", "date": "2023-07-07 20:16:34 +0200", "snippet": "Eine der faszinierendsten Attraktionen, die man auf den Kanarischen Inseln besuchen kann, ist ohne Zweifel der Loro Parque. Dieser Zoo, der ursprünglich als Papageienpark begann, hat sich zu einem ...", "content": "Eine der faszinierendsten Attraktionen, die man auf den Kanarischen Inseln besuchen kann, ist ohne Zweifel der Loro Parque. Dieser Zoo, der ursprünglich als Papageienpark begann, hat sich zu einem echten Tierparadies entwickelt, das eine Vielzahl von Arten beherbergt. Von den kleinsten Insekten bis hin zu den größten Säugetieren, der Loro Parque hat sie alle. Der Park ist ein Muss für jeden, der Teneriffa besucht, und bietet eine Vielzahl von Aktivitäten, die für die ganze Familie geeignet sind. Wir haben den Park besucht und möchten unsere Erfahrungen mit Ihnen teilen.Unser Besuch begann mit den namensgebenden Stars des Parks - den Papageien. Ihre leuchtenden Farben und spielerischen Persönlichkeiten waren ein sofortiger Stimmungsaufheller. Von den kleinen, lebhaften Aras bis hin zu den majestätischen Graupapageien, jedes Individuum schien seine eigene Persönlichkeit zu haben.Der Loro Parque ist jedoch weit mehr als nur ein Papageienpark. Eines der Highlights unseres Besuchs waren die hypnotisierenden Quallen. Ihr sanftes Gleiten durch das Wasser war fast meditativ zu beobachten. Die Vielfalt der Arten war beeindruckend und die Farben und Formen waren wunderschön.Als Nächstes haben wir die beeindruckenden Schildkröten bewundert. Diese ruhigen, geduldigen Kreaturen strahlten eine Art alte Weisheit aus, die uns in Ehrfurcht versetzte. Sie waren so groß, dass sie fast wie kleine Dinosaurier aussahen.Kein Besuch im Loro Parque wäre vollständig ohne einen Besuch bei den Flamingos. Mit ihrem ausgeprägten pinkfarbenen Gefieder und ihrer eleganten Haltung sind sie immer ein Blickfang. Wir haben uns die Zeit genommen, sie zu beobachten, wie sie sich in der Sonne aalen und sich gegenseitig mit ihren langen Hälsen putzen. Es war ein wunderschöner Anblick.Unsere Tour durch den Park endete mit den Pinguinen. Trotz der tropischen Umgebung des Parks haben die Pinguine dank eines speziell für sie konzipierten Lebensraums ein kühles und komfortables Zuhause. Es war ein unvergesslicher Anblick, sie fröhlich durch ihr frostiges Paradies watscheln zu sehen. Aktuell ist in der Antarkis Winter, deswegen ist das Licht stark gedimmt und die Pinguine sind nicht so aktiv. Aber es war trotzdem ein schöner Anblick.Der Loro Parque ist nicht nur ein Ort der Unterhaltung, sondern auch ein Ort des Engagements für den Tierschutz und die Artenerhaltung. Es war eine Freude zu sehen, wie gut für die Tiere gesorgt wird und welche Bemühungen unternommen werden, um die Vielfalt unseres Planeten zu erhalten. Unser Besuch im Loro Parque war definitiv ein Höhepunkt unserer Reise zu den Kanarischen Inseln." }, { "title": "Erstellen von Einladungslinks zur Benutzerauthentifizierung in einer mobilen App", "url": "/posts/pyhton-implementieren-eines-einladungslinks/", "categories": "Programming, Python", "tags": "asynchronous programming, networking, token, python", "date": "2023-07-07 07:45:00 +0200", "snippet": "In diesem Tutorial gehen wir durch den Prozess der Erstellung von Einladungslinks, die es Benutzern ermöglichen, sich in Ihrer App zu authentifizieren und bestimmten Benutzergruppen beizutreten.Vor...", "content": "In diesem Tutorial gehen wir durch den Prozess der Erstellung von Einladungslinks, die es Benutzern ermöglichen, sich in Ihrer App zu authentifizieren und bestimmten Benutzergruppen beizutreten.Vorbereitung Sie benötigen eine mobile App, die mit Apple SignIn integriert ist. Ein Backend-Server, der in der Lage ist, JWT (JSON Web Token) zu generieren und zu validieren. In diesem Tutorial verwenden wir Quart, ein Python-Web-Framework. Ihre App muss eine Web-Domain haben, die Apple verifizieren kann (z.B. https://api0.example.com), und Sie müssen in der Lage sein, die apple-app-site-association (AASA) Datei dort bereitzustellen.Schritt 1: Erstellen der Einladungslink-Route auf dem Backend-ServerDie erste Aufgabe besteht darin, eine neue Route auf dem Backend-Server zu erstellen, die /get-invite-link genannt werden könnte. Diese Route generiert ein JWT, das Informationen über die Gruppen-ID enthält, zu der der Benutzer eingeladen wird, und sendet dieses Token zurück an die mobile App.@app.route('/get-invite-link', methods=['POST'])async def get_invite_link(): data = await request.get_json() group_id = data.get('group_id') if not group_id: return await make_response(jsonify({'error': 'Missing group_id'}), 400) # Create JWT payload payload = { 'group_id': group_id, 'exp': datetime.utcnow() + timedelta(hours=8) # 8 hours of validity } invite_token = jwt.encode(payload, JWT_SECRET, algorithm='HS256') # For the simplicity, let's assume your app's URL is https://app.example.com invite_link = f\"https://app.example.com/invite/{invite_token}\" return await make_response(jsonify({'invite_link': invite_link}), 200) Hinweis: In diesem Beispiel verwenden wir Quart, ein Python-Web-Framework, um die Route zu erstellen. Wenn Sie ein anderes Framework verwenden, müssen Sie die entsprechenden Methoden verwenden, um die Anfrage zu erhalten und die Antwort zu senden.Schritt 2: Erstellen des Einladungslinks in der mobilen AppIn der mobilen App erstellen Sie einen “Einladungslink erstellen” -Button, der eine Anfrage an die /get-invite-link-Route des Backend-Servers sendet und die Gruppen-ID übergibt. Der Server wird ein JWT generieren und zurück an die App senden. Die App wird dann die URL des Einladungslinks erstellen, indem sie das Token an die Basis-URL der App anhängt.Schritt 3: Teilen des EinladungslinksIn der App können Sie nun den Einladungslink über den gewünschten Kommunikationskanal teilen (z.B. E-Mail, Messaging-Apps, etc.).Schritt 4: Verwenden des EinladungslinksWenn der eingeladene Benutzer auf den Einladungslink klickt, wird die App geöffnet und die App sendet eine Anfrage an eine andere Route auf dem Server, z.B. /use-invite-link, und übergibt das Einladungs-Token. Der Server wird das Token validieren, die Benutzer-ID extrahieren und den Benutzer zur entsprechenden Gruppe hinzufügen.@app.route('/use-invite-link', methods=['POST'])async def use_invite_link(): data = await request.get_json() user_id = data.get('user_id') invite_token = data.get('invite_token') if not user_id or not invite_token: return await make_response(jsonify({'error': 'Missing user_id or invite_token'}), 400) try: payload = jwt.decode(invite_token, JWT_SECRET, algorithms=['HS256']) except jwt.ExpiredSignatureError: return await make_response(jsonify({'error': 'Invite token expired'}), 400) except jwt.InvalidTokenError: return await make_response(jsonify({'error': 'Invalid invite token'}), 400) group_id = payload.get('group_id') # Let's assume user_db_manager has a method to add user to a group user_db_manager = current_app.config['USERDB_MANAGER'] await user_db_manager.add_user_to_group(user_id, group_id) return await make_response(jsonDurch die Verwendung von JWT und Apple SignIn können Sie sicherstellen, dass nur authentifizierte Benutzer eingeladen werden und beitreten können. Beachten Sie, dass das Token eine begrenzte Gültigkeitsdauer hat (in unserem Beispiel 8 Stunden), um die Sicherheit zu erhöhen.Dieses Tutorial zeigt, wie Sie Einladungslinks in Ihrer App implementieren können, um Benutzern den Beitritt zu bestimmten Benutzergruppen zu ermöglichen. Mit einigen Anpassungen können Sie es auf die spezifischen Anforderungen Ihrer App abstimmen. Viel Spaß beim Programmieren!" }, { "title": "Die leisen Begleiter: Eine Geschichte über Eidechsen am El Teide", "url": "/posts/urlaub-luz-die-eidechse/", "categories": "Reisen, Teneriffa", "tags": "teneriffa, el teide, eidechse, tiere", "date": "2023-07-06 19:30:00 +0200", "snippet": "Es gibt viele Dinge, die man auf einer Wanderung um den El Teide erwarten kann - die imposante Landschaft, die duftenden Pinienwälder, das prickelnde Gefühl der Aufregung, während man den vulkanisc...", "content": "Es gibt viele Dinge, die man auf einer Wanderung um den El Teide erwarten kann - die imposante Landschaft, die duftenden Pinienwälder, das prickelnde Gefühl der Aufregung, während man den vulkanischen Pfaden folgt. Doch es gibt auch Begegnungen, die vielleicht weniger erwartet, aber ebenso faszinierend sind. Heute möchte ich eine solche Geschichte erzählen, die Geschichte unserer kleinen, stillen Begleiter auf dieser Reise - den Eidechsen von Teneriffa.Die Eidechsen von Teneriffa sind bekannt für ihre Anpassungsfähigkeit und ihre Fähigkeit, in den rauesten Umgebungen zu überleben. Und so trafen wir während unserer Wanderung rund um den El Teide auf unzählige dieser faszinierenden Kreaturen. Sie sonnten sich auf Felsen, huschten über den Pfad und beobachteten uns mit neugierigen Augen.Eines dieser kleinen Wesen hat einen besonderen Eindruck hinterlassen. Wir nannten sie “Luz”, das spanische Wort für Licht. Luz war eine kleine, lebendige Eidechse mit schillernden grünen und blauen Schuppen, die im Sonnenlicht aufblitzten. Sie schien eine besondere Faszination für uns zu haben und folgte uns auf einem großen Teil unseres Weges.Es war erstaunlich, zu sehen, wie geschickt Luz sich über die felsige Landschaft bewegte, wie sie sich blitzschnell unter Steinen versteckte oder in der Sonne aalte. Sie schien die raue vulkanische Umgebung des El Teide perfekt zu beherrschen. Sie war ein kleiner, lebhafter Kontrast zu der Weite und Stille der Landschaft um uns herum.Am Ende des Tages, als die Sonne sich zu verabschieden begann und die Schatten der Pinien länger wurden, verloren wir Luz aus den Augen. Doch die Erinnerung an sie und die vielen anderen Eidechsen, die unseren Weg gekreuzt haben, bleibt uns.Abschied von einem unvergesslichen TagDie Begegnung mit den Eidechsen am El Teide hat uns daran erinnert, wie unglaublich vielfältig und anpassungsfähig das Leben ist. Sie hat uns gezeigt, dass es oft die kleinen Dinge sind, die eine Reise unvergesslich machen - die leisen Begegnungen, die überraschenden Momente, die lebhaften Farbtupfer in der Landschaft.Unsere Wanderung am El Teide war eine Reise voller Entdeckungen, und die Begegnung mit den Eidechsen war ein Teil davon. Sie sind Teil der Erinnerung an diesen Tag, Teil der Geschichte dieser Reise und Teil der Magie, die Teneriffa für uns so unvergesslich gemacht hat." }, { "title": "Improving Location Tracking in an iOS Application: Case Study of the Parkplatzmanager App", "url": "/posts/swift-improving-location-tracking/", "categories": "Programming, iOS", "tags": "ios, combine, networking, app optimization, location tracking, swiftui, swift, case study, parkplatzmanager", "date": "2023-07-04 16:45:00 +0200", "snippet": "In today’s blog post, we’ll be exploring how we can leverage Apple’s SwiftUI and Combine frameworks to improve location tracking within our iOS apps. Our use case involves a parking management app,...", "content": "In today’s blog post, we’ll be exploring how we can leverage Apple’s SwiftUI and Combine frameworks to improve location tracking within our iOS apps. Our use case involves a parking management app, where we want to keep track of the user’s current location to provide accurate services.Initial StateInitially, our code had a lack of continuous location updates while the user was interacting with the app. This could potentially lead to inaccurate location data, which would affect the functionality of our parking management app.struct ParkingView: View { @EnvironmentObject var locationService: LocationService @State private var currentLocation: CLLocation? var body: some View { // Other views... .onAppear { os_log(\"Location updates started\", log: viewLog, type: .debug) locationService.startLocationUpdates() .sink { location in self.currentLocation = location os_log(\"Current location: %@\", log: viewLog, type: .debug, location.description) } } }}Continuous Location UpdatesTo make location tracking more accurate, we decided to implement continuous location updates.First, we made changes in LocationService by replacing the getCurrentLocation() method with startLocationUpdates(). This method would return a publisher emitting new locations as they were updated by the CLLocationManager.class LocationService: NSObject, ObservableObject { private let locationManager = CLLocationManager() private var locationSubject = PassthroughSubject&lt;CLLocation, Never&gt;() func startLocationUpdates() -&gt; AnyPublisher&lt;CLLocation, Never&gt; {\t locationManager.startUpdatingLocation()\t return locationSubject.eraseToAnyPublisher() }}Next, we modified the SwiftUI View’s .onAppear method to subscribe to this publisher, storing the returned AnyCancellable in a @State property. This is important because we need to manage the lifecycle of the subscription and make sure it gets cancelled when the view disappears.In the .onAppear method, we started the location updates and stored the new location updates in currentLocation:.onAppear { self.locationUpdateCancellable = self.locationService.startLocationUpdates() .sink { location in self.currentLocation = location }}To make sure that location updates stop when the view is no longer on screen, we added an .onDisappear method that cancels the subscription and stops location updates:.onDisappear { self.locationService.stopUpdatingLocation() self.locationUpdateCancellable?.cancel()}Enhanced Button ActionsWe also improved the action buttons within the app. Each button has a role-based action which uses the camera to scan VIN numbers and license plates, and then sends the updated status of the vehicle to the backend.Here is an anonymized example of an action button within the ParkingView:private func actionButton(title: String, iconName: String, statusText: String, action: @escaping () -&gt; Void) -&gt; some View { Button(action: { AVCaptureDevice.requestAccess(for: .video) { granted in if granted { action() self.vehicleStatus = statusText self.isShowingScannerModal = true } else { os_log(\"Camera access not granted\", log: errorLog, type: .error) } } }) { HStack { Image(systemName: \"qrcode\") Text(title) Image(systemName: iconName) } } .padding() .background(Color.blue) .foregroundColor(.white) .cornerRadius(10)}ConclusionWith these changes, our location tracking is now more accurate and efficient. The use of Combine allowed us to reactively update our app’s state based on the user’s location, and SwiftUI made it easy to manage the lifecycle of these updates. Additionally, the role-based actions provide a user-friendly way to update vehicle statuses within the parking management app.[YouTube Video Placeholder]In our next post, we’ll explore further improvements we can make to our parking management app using more advanced SwiftUI and Combine techniques. Stay tuned!tags: [SwiftUI, Combine, iOS, Location Services]" }, { "title": "Optimizing an iOS Application: Case Study of the Parkplatzmanager App", "url": "/posts/swift-optimizing-ios-app/", "categories": "Programming, iOS", "tags": "ios, networking, app optimization, location tracking, combine, swift", "date": "2023-07-04 11:45:00 +0200", "snippet": "In the last couple of days, we’ve embarked on an exciting journey of improving and optimizing an iOS application, the parkplatzmanager app. In this blog post, we will walk you through some of the c...", "content": "In the last couple of days, we’ve embarked on an exciting journey of improving and optimizing an iOS application, the parkplatzmanager app. In this blog post, we will walk you through some of the changes we made and how they led to a more efficient and well-structured app.IntroductionThe parkplatzmanager app had been initially developed with a basic architecture, and as it grew in complexity, there were areas in the code that needed optimization and restructuring. Some of the issues we identified included ambiguous type inference, a lack of email validation, handling cache expiration, and dealing with data inconsistencies due to incorrect key-value mapping in the JSON response.Let’s walk through how we tackled each of these issues.1. Resolving Ambiguous Type InferenceSwift, like many other languages, performs type inference to determine the type of an expression when it’s not explicitly provided. However, sometimes the compiler might face difficulties in inferring the type of an expression, especially when multiple type possibilities exist. We encountered such an issue in the ParkingService.swift file:return Promise { seal in firstly { RealDataRepository.shared.fetchUserData(userId: userId, groupId: groupId) }.done { vehicleData in do { // Process the data seal.fulfill(()) } catch let error { seal.reject(error) } }.catch { error in seal.reject(error) }}Here, Swift was unable to infer the type of vehicleData. To resolve this, we explicitly provided the type of the vehicleData:RealDataRepository.shared.fetchUserData(userId: userId, groupId: groupId)}.done { (vehicleData: [Vehicle]) in do { // Process the data seal.fulfill(()) } catch let error { seal.reject(error) }2. Enforcing Email ValidationIn the InviteView.swift file, we observed that the app did not validate emails, potentially leading to incorrect or invalid email addresses being accepted.We first introduced a method isValidEmail(_ email: String) -&gt; Bool which was initially left as a placeholder. Later, we added a simple email validation logic:private func isValidEmail(_ email: String) -&gt; Bool { let emailRegex = \"[A-Z0-9a-z._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\" let emailTest = NSPredicate(format:\"SELF MATCHES %@\", emailRegex) return emailTest.evaluate(with: email)}Furthermore, we made the text field in SwiftUI ignore auto-capitalization, ensuring that only lower case characters are allowed:TextField(\"Email\", text: $email) .keyboardType(.emailAddress) .autocapitalization(.none)3. Handling Cache ExpirationOur app uses Redis for caching, and we identified a need to set a time-to-live (TTL) or expiration for our keys to control the freshness of the data. This can be done using the EXPIRE command in Redis:await redis.set(cache_key, json.dumps(vehicle))await redis.expire(cache_key, 300) # Set a TTL of 300 seconds4. Addressing JSON Key-Value Mapping InconsistenciesThe application experienced crashes due to mismatches between the expected and received JSON data, particularly a missing vehicle key. We realized that the received JSON’s structure was not what the app expected.To fix this, we updated our model to matchthe actual data received, ensuring that the keys in the Swift model matched the keys in the JSON data.struct ResponseData: Codable { let status: String let data: [Vehicle]}struct Vehicle: Codable { let _id: String let fullName: String? let groupId: String let latitude: Double let licensePlate: String let longitude: Double let timestamp: String let userId: String let vehicleStatus: String let vin: String? enum CodingKeys: String, CodingKey { case _id case fullName case groupId = \"group_id\" case latitude case licensePlate = \"license_plate\" case longitude case timestamp case userId = \"user_id\" case vehicleStatus = \"vehicle_status\" case vin }}ConclusionOptimizing an app often involves refactoring and improving the existing code to ensure it’s efficient, scalable, and maintainable. In our case, we were able to solve type inference ambiguity, improve data validation, handle cache expiration, and deal with JSON key-value inconsistencies. These changes significantly improved the parkplatzmanager app’s robustness and efficiency, and we hope this walkthrough was insightful and helpful!" }, { "title": "Grundlagen zur Nutzung von Github Teil 2", "url": "/posts/github-einfuehrung-teil2/", "categories": "Github", "tags": "git, commit, branch, merge, clone, push, pull", "date": "2023-07-03 17:00:00 +0200", "snippet": "Titel: “Grundlagen von GitHub: Von Repositories bis zu Commits und Branches”Ein weiterer wichtiger Aspekt von GitHub sind die “Branches” und “Merges”. Diese Konzepte sind essenziell für das Verstän...", "content": "Titel: “Grundlagen von GitHub: Von Repositories bis zu Commits und Branches”Ein weiterer wichtiger Aspekt von GitHub sind die “Branches” und “Merges”. Diese Konzepte sind essenziell für das Verständnis, wie die Entwicklung in Git organisiert wird.Branches in GitHubEin “Branch” ist eine unabhängige Linie der Entwicklung in Ihrem Projekt. Stellen Sie sich das wie einen parallelen Pfad vor, auf dem Sie arbeiten können, ohne die Hauptlinie der Entwicklung (üblicherweise “master” oder “main” genannt) zu beeinflussen. Jeder Branch hat seinen eigenen Verlauf von Commits und kann unabhängig von anderen Branches bearbeitet werden.Ein neuer Branch wird oft erstellt, wenn Sie an einer neuen Funktion oder einem Bugfix arbeiten. So können Sie Änderungen vornehmen und testen, ohne den Hauptcode zu beeinträchtigen. Der Befehl git branch zeigt Ihnen alle vorhandenen Branches an, und mit git branch &lt;branch-name&gt; können Sie einen neuen Branch erstellen.Merges in GitHubWenn Sie Ihre Arbeit in einem Branch abgeschlossen haben und diese Änderungen in den Hauptzweig einfließen lassen wollen, führen Sie einen “Merge” durch. Bei einem Merge werden die Änderungen aus einem Branch in einen anderen übertragen.Der Befehl git merge &lt;branch-name&gt; führt den angegebenen Branch in den aktuell aktiven Branch ein. Eventuelle Konflikte, die dabei auftreten (z.B. wenn in beiden Branches die gleiche Zeile in einer Datei geändert wurde), müssen manuell gelöst werden.FazitBranches und Merges sind zentrale Bestandteile der Arbeit mit Git und GitHub. Sie ermöglichen paralleles Arbeiten und erleichtern die Integration von Änderungen. Mit der Zeit werden Sie feststellen, dass die Nutzung von Branches und Merges den Arbeitsfluss erheblich erleichtern kann.Wir hoffen, dass dieser Blogbeitrag Ihnen dabei hilft, den vollen Nutzen aus GitHub zu ziehen. Bei weiteren Fragen oder Unklarheiten, zögern Sie nicht, einen Kommentar zu hinterlassen oder uns direkt zu kontaktieren. Viel Spaß beim Coden!" }, { "title": "Grundlagen zur Nutzung von Github Teil 1", "url": "/posts/github-einfuehrung-teil1/", "categories": "Github", "tags": "git, branch, commit, repository", "date": "2023-07-03 09:00:00 +0200", "snippet": "Die Nutzung von GitHub: Ein umfassender LeitfadenGitHub ist eine webbasierte Hosting-Plattform für die Versionsverwaltung Git. Es bietet eine Reihe von Funktionen, die das Arbeiten in Gruppen erlei...", "content": "Die Nutzung von GitHub: Ein umfassender LeitfadenGitHub ist eine webbasierte Hosting-Plattform für die Versionsverwaltung Git. Es bietet eine Reihe von Funktionen, die das Arbeiten in Gruppen erleichtern und zur gemeinsamen Entwicklung von Software-Projekten genutzt werden. In diesem Blog-Beitrag gehen wir tief in die Nutzung von GitHub ein und erklären Begriffe wie Repository, Commit, Pull, Push, Stash und vieles mehr.GitHub RepositoryEin Repository (oder “Repo”) ist ein Speicherort für ein Projekt. Es enthält alle Projektdateien und jede Änderung, die an diesen Dateien vorgenommen wird. Ein GitHub-Repository enthält nicht nur den Projektcode, sondern auch die Versionshistorie und weitere Informationen wie Issues, Projektboards und Aktionen.Erstellen Sie ein neues Repository, indem Sie auf der GitHub-Hauptseite auf die Schaltfläche “New” klicken. Geben Sie Ihrem Repository einen Namen und eine Beschreibung. Sie können es öffentlich machen, damit jeder es sehen und daran arbeiten kann, oder privat, damit nur Sie und die von Ihnen eingeladenen Personen darauf zugreifen können.Git CommitEin “Commit” ist eine Änderung, die Sie an den Dateien in Ihrem Repository vornehmen. Jeder Commit hat eine eindeutige ID, die Sie verwenden können, um auf spezifische Änderungen zu verweisen. Ein Commit enthält auch eine Commit-Nachricht, die beschreibt, was in diesem Commit geändert wurde.Um einen Commit zu erstellen, machen Sie zuerst Änderungen an den Dateien in Ihrem Repository. Wenn Sie fertig sind, können Sie mit dem Befehl git add die geänderten Dateien zur “Staging Area” hinzufügen. Danach können Sie mit git commit -m \"Ihre Nachricht\" einen neuen Commit mit Ihrer Änderung erstellen.Git PushMit dem Befehl “Push” laden Sie Ihre lokalen Änderungen in Ihr GitHub-Repository hoch. Der Befehl git push origin master beispielsweise lädt alle Änderungen, die Sie in Ihrem lokalen “master”-Branch gemacht haben, auf GitHub hoch.Git Pull“Pull” ist der umgekehrte Vorgang zu “Push”. Mit einem “Pull” holen Sie die neuesten Änderungen aus Ihrem GitHub-Repository und aktualisieren damit Ihren lokalen Code. Der Befehl dazu lautet git pull origin master, um die neuesten Änderungen vom “master”-Branch herunterzuladen.Git StashMit dem Befehl git stash können Sie Änderungen, die Sie noch nicht committen möchten, vorübergehend beiseitelegen. Diese Änderungen werden gespeichert und können später wieder hervorgeholt werden. Das ist besonders nützlich, wenn Sie mitten in der Arbeit an einer Funktion sind und schnell zu einem anderen Branch wechseln müssen.FazitDas Arbeiten mit GitHub kann zu Beginn kompliziert erscheinen, aber mit ein wenig Übung wird es schnell zu einem mächtigen Werkzeug für die Softwareentwicklung. Die Befehle und Konzepte, die wir in diesem Blogbeitrag besprochen haben, sind die Grundlage für die Arbeit mit GitHub. Sie ermöglichen es Ihnen, Ihre Projekte effektiv zu verwalten, zusammen mit anderen zu arbeiten undden Überblick über Ihre Änderungen zu behalten.Wir hoffen, dass dieser Beitrag Ihnen einen guten Überblick über die Grundlagen von GitHub gegeben hat. Viel Spaß beim Coden!" }, { "title": "Einrichtung und Fehlerbehebung eines Lychee-Bilderservers", "url": "/posts/linux-lychee-image-server-copy/", "categories": "Homelab, VMs", "tags": "servers, nginx, webserver, lychee, php, mysql, postgresql, sqlite3, linux", "date": "2023-07-03 01:00:00 +0200", "snippet": "Lychee ist eine großartige Open-Source-Lösung zur Verwaltung und Organisation Ihrer Fotos. In diesem Beitrag zeige ich Ihnen, wie Sie einen Lychee-Bilderserver einrichten und potenzielle Probleme b...", "content": "Lychee ist eine großartige Open-Source-Lösung zur Verwaltung und Organisation Ihrer Fotos. In diesem Beitrag zeige ich Ihnen, wie Sie einen Lychee-Bilderserver einrichten und potenzielle Probleme beheben können.Schritt 1: SystemvoraussetzungenUm Lychee zu installieren, stellen Sie sicher, dass Ihr Server folgendes vorweist: Ein Webserver wie Apache oder nginx. Eine Datenbank: MySQL (version &gt; 5.7.8), MariaDB (version &gt; 10.2), PostgreSQL (version &gt; 9.2), oder SQLite3. PHP &gt;= 8.0 mit entsprechenden PHP-Erweiterungen. Imagick-Extension für bessere Thumbnail-Generierung.Schritt 2: Lychee herunterladen und installierenBeginnen wir mit dem Herunterladen von Lychee. Sie können Lychee direkt von GitHub klonen:git clone https://github.com/LycheeOrg/Lychee.gitWechseln Sie in das Verzeichnis und installieren Sie die erforderlichen Abhängigkeiten mit Composer:cd Lycheecomposer install --no-devSchritt 3: Datenbank konfigurierenErstellen Sie eine neue Datenbank für Lychee und behalten Sie die Zugangsdaten bei. Sie benötigen diese, wenn Sie Lychee zum ersten Mal starten.mysql -u root -pCREATE DATABASE lychee;CREATE USER 'lychee'@'localhost' IDENTIFIED BY 'password';GRANT ALL PRIVILEGES ON lychee.* TO 'lychee'@'localhost';FLUSH PRIVILEGES;exit;Schritt 4: Nginx-KonfigurationHier ist ein einfaches Beispiel für eine Nginx-Konfigurationsdatei:nginxserver { listen 80; server_name your-domain.com; root /path/to/your/lychee/public; index index.php; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass unix:/var/run/php/php8.0-fpm.sock; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PHP_VALUE \"post_max_size=100M upload_max_filesize=20M \"; }}Vergessen Sie nicht, Ihren Domainnamen und den Pfad zu Ihrem Lychee-Verzeichnis zu ändern. Nachdem Sie Ihre Konfigurationsdatei eingerichtet haben, vergewissern Sie sich, dass Nginx und PHP-FPM korrekt konfiguriert sind und laufen.Schritt 5: Berechtigungen setzenVergewissern Sie sich, dass die Berechtigungen für Ihr Lychee-Verzeichnis korrekt gesetzt sind. Sie können dies mit den folgenden Befehlen erreichen:sudo chown -R www-data:www-data /var/www/Lychee/sudo find /var/www/Lychee/ -type d -exec chmod 775 {} \\;sudo find /var/www/Lychee/ -type f -exec chmod 664 {} \\;Schritt 6: Lychee konfigurieren und nutzenÖffnen Sie nun Ihren Webbrowser und navigieren Sie zu Ihrer Lychee-Website. Sie werden aufgefordert, eine Datenbank zu erstellen und einen Benutzernamen und ein Passwort zu wählen.TroubleshootingManchmal könnten Sie auf einige Probleme stoßen. Hier sind einige gängige Lösungen:Fehler 502: Dieser Fehler tritt normalerweise auf, wenn Nginx versucht, mit dem PHP-FPM-Dienst zu kommunizieren und keine Antwort erhält. Überprüfen Sie, ob PHP-FPM läuft (systemctl status php8.1-fpm.service) und ob der Socket-Pfad in der Nginx-Konfigurationsdatei korrekt ist.Berechtigungsprobleme: Wenn Sie eine Fehlermeldung erhalten, dass bestimmte Verzeichnisse oder Dateien die falschen Berechtigungen haben, führen Sie die oben genannten chown und chmod Befehle erneut aus.Fehler beim Hochladen von Bildern: Wenn Sie Probleme beim Hochladen von Bildern haben, überprüfen Sie die PHP-Einstellungen in Ihrer Nginx-Konfigurationsdatei. Stellen Sie sicher, dass post_max_size und upload_max_filesize hoch genug eingestellt sind.FazitMit diesen Schritten sollten Sie in der Lage sein, Lychee auf Ihrem Server zu installieren und zu konfigurieren. Bitte beachten Sie, dass die genaue Vorgehensweise je nach Ihrer spezifischen Serverkonfiguration variieren kann." }, { "title": "Fehlersuche beim Einrichten einer Datenbank-Verbindung in Quart", "url": "/posts/python-Fehlersuche-beim-Einrichten-einer-DB-Verbindung-in-Quart/", "categories": "Programming, Python", "tags": "quart, api, development, flask, database, python, async, await", "date": "2023-07-02 00:00:00 +0200", "snippet": "Fehlersuche beim Einrichten einer Datenbank-Verbindung in QuartDie Fehlermeldung KeyError: 'USER_DB_MANAGER' in einer Quart-Anwendung kann auftreten, wenn versucht wird, auf ein Konfigurationseleme...", "content": "Fehlersuche beim Einrichten einer Datenbank-Verbindung in QuartDie Fehlermeldung KeyError: 'USER_DB_MANAGER' in einer Quart-Anwendung kann auftreten, wenn versucht wird, auf ein Konfigurationselement zuzugreifen, das noch nicht in die Anwendungskonfiguration aufgenommen wurde. Im folgenden Artikel werden wir die möglichen Ursachen dieses Problems untersuchen und Vorschläge zur Lösung dieses Problems geben.ProblemDie Fehlermeldung sieht so aus:KeyError: 'USER_DB_MANAGER'Dieser Fehler weist darauf hin, dass der Schlüssel ‘USER_DB_MANAGER’ im aktuellen app-Konfigurationsobjekt nicht gefunden wird. Dies deutet darauf hin, dass die Initialisierung und Speicherung der Manager in der App-Konfiguration in der Startup-Routine Ihrer Anwendung möglicherweise nicht erfolgreich durchgeführt wurde.LösungStellen Sie sicher, dass die Initialisierung der Manager (einschließlich USER_DB_MANAGER) und ihre Speicherung in der App-Konfiguration vor den ersten Anfragen erfolgen. Dies geschieht typischerweise in der Setup-Funktion Ihrer App, die bei Start ausgeführt wird. Stellen Sie auch sicher, dass keine Exceptions während dieser Initialisierung ausgelöst werden, die dazu führen könnten, dass der Code zur Speicherung der Manager in der App-Konfiguration nicht erreicht wird.Ein Beispiel könnte wie folgt aussehen:@app.before_servingasync def startup(): \"\"\" This function is called before the app starts. It initializes the app container and the database managers. \"\"\" # Your code to initialize managers... user_db_manager = ... # Store managers into the app config for easy access app.config['USER_DB_MANAGER'] = user_db_managerSie können später in Ihrer Route darauf zugreifen:@user_api_blueprint.before_app_first_requestasync def setup(): user_db_manager = current_app.config['USER_DB_MANAGER'] # the rest of your code...FazitBei der Arbeit mit Quart und anderen asynchronen Web-Frameworks ist es wichtig, die Initialisierung von Ressourcen wie Datenbank-Verbindungen sorgfältig zu steuern. Stellen Sie sicher, dass alle erforderlichen Ressourcen korrekt initialisiert und in der Anwendungskonfiguration gespeichert sind, bevor Sie versuchen, sie in Ihren Anforderungshandlern zu verwenden." }, { "title": "Handling Asynchronous Tasks in iOS with Combine", "url": "/posts/swift-working-with-combine/", "categories": "Programming, iOS", "tags": "combine, ios, asynchronous, networking, swiftui, swift", "date": "2023-07-01 12:00:00 +0200", "snippet": "Working with Combine FrameworkIn modern iOS development, handling asynchronous tasks efficiently is crucial. While there are several ways to handle asynchronous tasks in Swift, including closures, ...", "content": "Working with Combine FrameworkIn modern iOS development, handling asynchronous tasks efficiently is crucial. While there are several ways to handle asynchronous tasks in Swift, including closures, delegates, and DispatchQueue, one powerful approach is to use the Combine framework introduced in iOS 13.The Combine framework allows you to work with asynchronous events as if they were sequences of values you can transform and manipulate using high-order functions like map, filter, reduce, etc.The example code we have been looking at makes extensive use of the Combine framework for network requests. Here’s a method that sends a network request and returns a publisher:func request&lt;T: Encodable, U: Decodable&gt;(endPoint: APIEndpoint, parameters: T?, authRequired: Bool = true) -&gt; AnyPublisher&lt;U, Error&gt; { guard let request = buildRequest(from: endPoint, with: parameters, authRequired: authRequired) else { return Fail(error: NetworkError.invalidURL) .eraseToAnyPublisher() } return session.dataTaskPublisher(for: request) .tryMap { data, response in // handle response here... return data } .decode(type: U.self, decoder: JSONDecoder()) .mapError { error in // handle error here... return NetworkError.unknownError(error) } .eraseToAnyPublisher()}In this method, a network request is initiated with session.dataTaskPublisher(for: request), which returns a publisher that emits the server’s response as a tuple of (Data, URLResponse).This is then transformed using tryMap to only keep the Data part and check the HTTP response status. The resulting data is then decoded into the expected response type U with .decode(type: U.self, decoder: JSONDecoder()).The mapError function maps any error that occurs during this process to our custom NetworkError type.The use of Combine here provides several benefits: Composability: The publisher returned by the request method can be further composed with other publishers to handle complex asynchronous workflows. Error handling: Errors are propagated along the publisher chain and can be caught and handled at any point. Code readability: The high-level, declarative syntax of Combine makes the code more readable and easier to understand compared to nested closures or delegate methods. Integration with Swift UI: Combine works seamlessly with Swift UI, allowing you to easily update your UI based on the results of network requests.map and tryMapThe map function is used to transform the output of a publisher. For example, if you have a publisher that emits integers, you could use map to transform them into strings:let intPublisher = Just(5)let stringPublisher = intPublisher.map { \"The number is \\($0)\" }The tryMap function is similar, but it can throw errors, allowing you to handle possible failures during the transformation process.filterThe filter function is used to emit only the values that satisfy a given predicate. For example, you could create a publisher that only emits even numbers like this:let numbers = [1, 2, 3, 4, 5, 6].publisherlet evenNumbers = numbers.filter { $0 % 2 == 0 }combineLatestThe combineLatest function is used when you need to combine the latest values of multiple publishers. It emits a value whenever any of its input publishers emit a value, combining the latest values from each one.let publisher1 = PassthroughSubject&lt;Int, Never&gt;()let publisher2 = PassthroughSubject&lt;String, Never&gt;()let combined = publisher1.combineLatest(publisher2)publisher1.send(1)publisher2.send(\"a\") // Emits (1, \"a\")publisher1.send(2) // Emits (2, \"a\")publisher2.send(\"b\") // Emits (2, \"b\")mergeThe merge function combines the outputs from multiple publishers into a single publisher. Unlike combineLatest, it does not wait for each publisher to emit a value, but emits values as soon as they arrive from any publisher.switchToLatestThe switchToLatest operator is used when you have a publisher of publishers and you want to transform it into a publisher that emits only the latest values from the latest publisher. This is particularly useful when working with asynchronous tasks like network requests.zipThe zip operator combines multiple publishers by pairing their values together. Unlike combineLatest, it emits a value only when all of its input publishers have emitted a value.These are just a few examples of the operators available in the Combine framework. By composing these operators together, you can express complex asynchronous workflows in a clear and concise way.However, as with any tool, it’s important to understand its strengths and limitations. While Combine is extremely powerful for handling asynchronous tasks, it also has a steep learning curve and requires a good understanding of Swift and functional programming concepts. For simpler tasks, other approaches like closures or the new async/await syntax introduced in Swift 5.5 might be more suitable.In the next part of this series, we will explore how to test Combine code and handle common pitfalls. Stay tuned!" }, { "title": "Building API Routes with Quart", "url": "/posts/pyhton-building-api-routes-with-quart/", "categories": "Programming, Python", "tags": "quart, api, development, python, flask, async, await", "date": "2023-06-30 19:00:00 +0200", "snippet": "Building API Routes with QuartIn this blog post, we’ll go over how to use the Quart library in Python to create API endpoints. Quart is a Python ASGI web microframework whose API is a superset of t...", "content": "Building API Routes with QuartIn this blog post, we’ll go over how to use the Quart library in Python to create API endpoints. Quart is a Python ASGI web microframework whose API is a superset of the Flask API, meaning that you can use Flask-like syntax and patterns with Quart.from quart import Quart, request, jsonify, make_responseapp = Quart(__name__)@app.route('/api/test', methods=['GET'])async def test_route(): return jsonify({\"message\": \"This is a test route\"}), 200app.run()User Registration and LoginHere are examples of routes for user registration and login, which handle HTTP POST requests and use the async/await syntax.@app.route('/user/register', methods=['POST'])async def register(): data = await request.get_json() username = data.get('username') password = data.get('password') # Registration logic goes here... return jsonify({\"message\": f\"User {username} registered successfully\"}), 200@app.route('/user/login', methods=['POST'])async def login(): data = await request.get_json() username = data.get('username') password = data.get('password') # Login logic goes here... return jsonify({\"message\": \"Login successful\"}), 200Error HandlingYou can also return error responses with appropriate HTTP status codes. Below is an example of error handling for a missing parameter.@app.route('/user/login', methods=['POST'])async def login(): data = await request.get_json() username = data.get('username') password = data.get('password') if not username or not password: return jsonify({\"error\": \"Missing username or password\"}), 400 # Rest of the login logic goes here... return jsonify({\"message\": \"Login successful\"}), 200JSON Web Tokens (JWT)Quart can be combined with other libraries such as PyJWT to handle JSON Web Tokens (JWT) for authentication. Here’s an example route that verifies an identity token.@app.route('/verifyIdentityToken', methods=['POST'])async def verify_identity_token(): data = await request.get_json() id_token = data.get('identity_token') # Token verification logic goes here... return await make_response(jsonify({'user_id': user_id, 'jwt_token': jwt_token}), 200)That’s it for this quick look at Quart. The library is a powerful tool for building API endpoints in Python, and it is especially useful when combined with other libraries to handle things like JWT for authentication." }, { "title": "Debugging Network Requests in iOS with Swift", "url": "/posts/swift-debugging-network-requests-in-ios/", "categories": "Programming, iOS", "tags": "combine, ios, asynchronous, networking, swift, swiftui", "date": "2023-06-29 13:00:00 +0200", "snippet": "Debugging Network Requests in iOS with SwiftIn this article, we are going to discuss an essential part of every mobile application that interacts with a backend server - network requests. Specifica...", "content": "Debugging Network Requests in iOS with SwiftIn this article, we are going to discuss an essential part of every mobile application that interacts with a backend server - network requests. Specifically, we’ll focus on how to debug network requests in an iOS application using Swift.While developing an iOS application, it’s very common to encounter issues with network requests such as invalid request formats, issues with authorization or unexpected server responses. Being able to effectively debug these network requests can save you a lot of time and effort.Let’s start with the most straightforward way of debugging - logging.LoggingOne of the most common ways to debug network requests is to log essential parts of the requests and responses to the console. This includes the request’s URL, method, headers, and body, as well as the response’s status code and body.Swift’s os_log API can be used for this purpose. It provides a powerful, flexible way to efficiently log diagnostic messages from your app.Here is a simple example of how you can use os_log to log network request and response details:os_log(\"Starting request to endpoint: %@\", log: self.log, type: .info, endPoint.path)...os_log(\"Built request: %@\", log: self.log, type: .debug, request.debugDescription)os_log(\"Request httpBody: %@\", log: self.log, type: .debug, String(data: request.httpBody ?? Data(), encoding: .utf8) ?? \"Not decodable to string\")os_log(\"Request headers: %@\", log: self.log, type: .debug, request.allHTTPHeaderFields?.description ?? \"None\")os_log(\"Request URL: %@\", log: self.log, type: .debug, request.url?.absoluteString ?? \"None\")...os_log(\"Received data: %@\", log: self.log, type: .debug, String(data: data, encoding: .utf8) ?? \"Not decodable to string\")os_log(\"HTTP error: %d\", log: self.log, type: .error, httpResponse.statusCode)However, it’s important to be careful with what you log. Do not log sensitive data like passwords, tokens, or personally identifiable information (PII). This is particularly relevant when working with JWT tokens or similar, which often carry sensitive data.Anonymizing Sensitive DataFor instance, you might want to log the identity token used for signing in with Apple. However, this token contains sensitive information and should not be logged in its entirety.You could instead log a part of the token, for example the first few and last few characters, and replace the rest with placeholders:let anonymizedToken = String(identityToken.prefix(8)) + \"...\" + String(identityToken.suffix(8))os_log(\"Verifying identity token: %@\", log: self.log, type: .info, anonymizedToken)This will give you enough information to identify individual tokens in the logs, without revealing sensitive information.Checking the Server LogsWhile the client logs can give you a lot of information about what’s happening on the device, sometimes the problem lies on the server side. If you have access to the server logs, you should also check them for any errors or warnings.Here’s a simple example of what you might find in the server logs:2023-06-30 21:01:47,838 - quartapp - INFO - Request to /verifyIdentityToken...2023-06-30 21:01:47,838 - quartapp - DEBUG - Request data: None2023-06-30 21:01:47,838 - quartapp - ERROR - Missing request bodyIn this case, the server log shows that the request body was missing from the request. This gives you a clear indication of what went wrong and where to look in your client code to fix the issue.In conclusion, debugging network requests might seem daunting at first, but with the right tools and practices, it becomes a manageable task. Remember to use logging effectively and always be mindful of security and privacy considerations when handling sensitive data. Happy debugging!Note: The code snippets used in this article are based on Swift 5 and the standard Swift and Foundation libraries available as of Xcode 13. Some details might differ in newer or older versions of Swift and Xcode." }, { "title": "Building an Audit Trail with Quart, MongoDB, and Redis", "url": "/posts/python-audit-trail/", "categories": "Programming, Python", "tags": "auditTrail, webDevelopment, backend, microservices, python, quart, mongodb, redis", "date": "2023-06-28 15:00:00 +0200", "snippet": "Building an Audit Trail with Quart, MongoDB, and RedisIn this post, we will explore how to build an Audit Trail system using Quart for Python, MongoDB, and Redis. This will involve implementing Mon...", "content": "Building an Audit Trail with Quart, MongoDB, and RedisIn this post, we will explore how to build an Audit Trail system using Quart for Python, MongoDB, and Redis. This will involve implementing MongoDB for data storage, Quart for the web server and API, and Redis for caching data. We will create several API routes for managing audit entries in a MongoDB database.Introduction to the ModulesOur code is organized in several modules. Each module is responsible for handling specific aspects of the audit trail system.The AuditTrailDBManager ClassThe AuditTrailDBManager is defined in app/common/managers/audittraildbmanager.py. This class is responsible for managing the operations of the AuditTrailDB, which is a MongoDB database that we use for storing our audit trail entries. It provides methods for connecting to the MongoDB server, inserting documents, finding documents based on a query, updating documents, deleting documents, and saving and loading model data.class AuditTrailDBManager(IMongoDB, IDependency): \"\"\" AuditTrailDBManager is a class for managing the AuditTrailDB operations.\"\"\" ...The AuditTrailDBManager uses the motor library for making asynchronous MongoDB operations.The Quart API RoutesWe have defined our API routes in app/quart/routes/mongo_api_routes.py. We use the Quart library for Python to create these routes. Quart allows us to create API routes in a manner similar to Flask, but with support for Python’s asyncio library for asynchronous operations.Our API has several routes: /apiv3/audit_trail/check-port: Checks if the API is running. /apiv3/audit_trail/entries: Supports the POST method for creating a new audit entry and the GET method for retrieving audit entries. /apiv3/audit_trail/entries/&lt;entry_id&gt;: Supports the PUT method for updating an audit entry with a given ID, and the DELETE method for deleting an audit entry with a given ID.Here is a code snippet for one of the routes:@audit_trail_api_blueprint.route('/entries', methods=['POST'])@verify_jwt_tokenasync def create_audit_entry(): \"\"\" Erstellt einen neuen Audit-Eintrag.\"\"\" ...Each route function uses the @verify_jwt_token decorator, which verifies the JWT token for user authentication.The API routes interact with the MongoDB database through the AuditTrailDataController.The AuditTrailDataController ClassThe AuditTrailDataController is responsible for controlling the data of the audit trail. It uses the AuditTrailDBManager to interact with the MongoDB database.The AuditTrailDataController is initialized before the first request to the Quart application is made:@audit_trail_api_blueprint.before_app_first_requestasync def init_audit_trail_data_controller(): \"\"\" Initialisiert den AuditTrailDataController\"\"\" ...Redis for CachingTo reduce the load on the MongoDB server and speed up our application, we use Redis for caching the results of the GET requests. We store the results of each unique query in Redis. The next time the same query is made, we return the cached results from Redis instead of querying the MongoDB database again.Here is a code snippet for theGET request route that uses Redis:@audit_trail_api_blueprint.route('/entries', methods=['GET'])@verify_jwt_tokenasync def get_audit_entries(): ... # Erhalte Redis-Instanz redis = await get_redis() # Konvertiere query dict in str für den Gebrauch mit Redis query_str = json.dumps(query, sort_keys=True) cached_entries = await redis.get(query_str) if cached_entries is not None: ... else: entries = await audittrail_data_controller.get_audit_entries(query) ...In this code snippet, we get the Redis instance and then convert the query dictionary into a string to use with Redis. If the query results are already cached in Redis, we return those results. Otherwise, we get the audit entries from MongoDB and store the results in Redis for future requests.ConclusionWe have discussed how to build an audit trail system using Quart for Python, MongoDB, and Redis. The system provides API routes for managing audit entries stored in a MongoDB database and uses Redis for caching query results. The AuditTrailDBManager class manages the MongoDB operations, and the AuditTrailDataController controls the audit trail data. This system is a good example of how to use Quart, MongoDB, and Redis together to create a highly efficient and scalable web application.In future posts, we’ll further dive into the usage of Quart and other technologies to build highly scalable microservices. Stay tuned!Aufbau eines Audit-Trail mit Quart, MongoDB und RedisIn diesem Beitrag werden wir untersuchen, wie man ein Audit Trail System mit Quart für Python, MongoDB und Redis erstellt. Dabei implementieren wir MongoDB für die Datenspeicherung, Quart für den Webserver und die API sowie Redis zur Zwischenspeicherung von Daten. Wir werden mehrere API-Routen erstellen, um Audit-Einträge in einer MongoDB-Datenbank zu verwalten.Einführung in die ModuleUnser Code ist in mehrere Module organisiert. Jedes Modul ist für bestimmte Aspekte des Audit Trail Systems zuständig.Die Klasse “AuditTrailDBManager”Der “AuditTrailDBManager” ist in “app/common/managers/audittraildbmanager.py” definiert. Diese Klasse ist für die Verwaltung der Operationen der “AuditTrailDB” zuständig, einer MongoDB-Datenbank, in der wir unsere Audit Trail Einträge speichern. Sie bietet Methoden zum Verbinden mit dem MongoDB-Server, zum Einfügen von Dokumenten, zum Suchen von Dokumenten basierend auf einer Abfrage, zum Aktualisieren von Dokumenten, zum Löschen von Dokumenten sowie zum Speichern und Laden von Modelldaten.class AuditTrailDBManager(IMongoDB, IDependency): \"\"\" AuditTrailDBManager ist eine Klasse zur Verwaltung der AuditTrailDB-Operationen.\"\"\" ...Der “AuditTrailDBManager” verwendet die Motor-Bibliothek für asynchrone MongoDB-Operationen.Die Quart-API-RoutenWir haben unsere API-Routen in “app/quart/routes/mongo_api_routes.py” definiert. Wir verwenden die Quart-Bibliothek für Python, um diese Routen zu erstellen. Quart ermöglicht es uns, API-Routen ähnlich wie Flask zu erstellen, unterstützt jedoch die asynchrone Funktionalität von Pythons asyncio-Bibliothek.Unsere API hat mehrere Routen: /apiv3/audit_trail/check-port: Überprüft, ob die API läuft. /apiv3/audit_trail/entries: Unterstützt die POST-Methode zum Erstellen eines neuen Audit-Eintrags und die GET-Methode zum Abrufen von Audit-Einträgen. /apiv3/audit_trail/entries/&lt;entry_id&gt;: Unterstützt die PUT-Methode zum Aktualisieren eines Audit-Eintrags mit einer bestimmten ID und die DELETE-Methode zum Löschen eines Audit-Eintrags mit einer bestimmten ID.Hier ist ein Code-Schnipsel für eine der Routen:@audit_trail_api_blueprint.route('/entries', methods=['POST'])@verify_jwt_tokenasync def create_audit_entry(): \"\"\" Erstellt einen neuen Audit-Eintrag.\"\"\" ...Jede Routenfunktion verwendet den Dekorator “@verify_jwt_token”, der das JWT-Token für die Benutzerauthentifizierung überprüft.Die API-Routen interagieren über den “AuditTrailDataController” mit der MongoDB-Datenbank.Die Klasse “AuditTrailDataController”Der “AuditTrailDataController” ist für die Steuerung der Daten des Audit Trails zuständig. Er verwendet den “AuditTrailDBManager”, um mit der MongoDB-Datenbank zu interagieren.Der “AuditTrailDataController” wird initialisiert, bevor die erste Anfrage an die Quart-Anwendung gestellt wird:@audit_trail_api_blueprint.before_app_first_requestasync def init_audit_trail_data_controller(): \"\"\" Initialisiert den AuditTrailDataController.\"\"\" ...Redis zur ZwischenspeicherungUm die Belastung des MongoDB-Servers zu reduzieren und unsere Anwendung zu beschleunigen, verwenden wir Redis zur Zwischenspeicherung der Ergebnisse der GET-Anfragen. Wir speichern die Ergebnisse jeder eindeutigen Abfrage in Redis. Beim nächsten Mal, wenn dieselbe Abfrage gestellt wird, geben wir die zwischengespeicherten Ergebnisse aus Redis zurück, anstatt die MongoDB-Datenbank erneut abzufragen.Hier ist ein Code-Schnipsel für die GET-Anfrage-Route, die Redis verwendet:@audit_trail_api_blueprint.route('/entries', methods=['GET'])@verify_jwt_tokenasync def get_audit_entries(): ... # Hole Redis-Instanz redis = await get_redis() # Konvertiere query dict in str für die Verwendung mit Redis query_str = json.dumps(query, sort_keys=True) cached_entries = await redis.get(query_str) if cached_entries is not None: ... else: entries = await audittrail_data_controller.get_audit_entries(query) ...In diesem Code-Schnipsel holen wir die Redis-Instanz und konvertieren dann das Abfrage-Dictionary in einen String, um es mit Redis zu verwenden. Wenn die Abfrageergebnisse bereits in Redis zwischengespeichert sind, geben wir diese Ergebnisse zurück. Andernfalls holen wir die Audit-Einträge aus MongoDB und speichern die Ergebnisse in Redis für zukünftige Anfragen.FazitWir haben besprochen, wie man ein Audit Trail System mit Quart für Python, MongoDB und Redis erstellt. Das System bietet API-Routen zur Verwaltung von Audit-Einträgen, die in einer MongoDB-Datenbank gespeichert sind, und verwendet Redis zur Zwischenspeicherung von Abfrageergebnissen. Die Klasse “AuditTrailDBManager” verwaltet die MongoDB-Operationen und der “AuditTrailDataController” steuert die Audit Trail Daten. Dieses System ist ein gutes Beispiel dafür, wie man Quart, MongoDB und Redis zusammen verwendet, um eine hoch effiziente und skalierbare Webanwendung zu erstellen.In zukünftigen Beiträgen werden wir noch weiter auf den Einsatz von Quart und anderen Technologien zur Entwicklung hoch skalierbarer Microservices eingehen. Bleiben Sie dran!" }, { "title": "Pyhton Quart Hypercorn", "url": "/posts/pyhton-quart-hypercorn/", "categories": "", "tags": "", "date": "2023-06-27 00:00:00 +0200", "snippet": "Erstellen einer Webanwendung mit Quart und Hypercorn in PythonIm heutigen Blogpost möchten wir uns auf das Erstellen von Webanwendungen mit Quart und Hypercorn in Python konzentrieren. Quart ist ei...", "content": "Erstellen einer Webanwendung mit Quart und Hypercorn in PythonIm heutigen Blogpost möchten wir uns auf das Erstellen von Webanwendungen mit Quart und Hypercorn in Python konzentrieren. Quart ist ein leistungsfähiges, asynchrones Web-Framework für Python, während Hypercorn ein ASGI-Server ist, der dazu dient, Ihre Quart-Anwendung zu hosten.Die Quart-Anwendung, die wir in diesem Beitrag erstellen, integriert mehrere Technologien und Konzepte, darunter Redis, RabbitMQ und PostgreSQL. Dieser Beitrag geht davon aus, dass Sie mit Python und Grundlagen der Webentwicklung vertraut sind.Die Hauptanwendung erstellenZuerst importieren wir die benötigten Module. Quart ist das Hauptmodul, das wir verwenden werden, um unsere Webanwendung zu erstellen. Wir importieren auch mehrere Blueprints, die unsere Anwendungsrouten definieren:# quartapp.pyimport asynciofrom quart import Quart from hypercorn.asyncio import serve from hypercorn.config import Config from quart_redis import RedisHandler from app.common.config import Configfrom app.common.appcontainer import AppContainerfrom app.quart.routes import (metrics_api_blueprint, data_api_blueprint, rabbitmq_api_blueprint, cors_blueprint, user_api_blueprint, group_api_blueprint, audit_trail_api_blueprint)In diesem Codeblock sehen Sie, dass wir verschiedene Blueprints importieren, die die verschiedenen Teile unserer Anwendung definieren. Ein Blueprint in Quart ist ein Modul, das mehrere Routen, Vorlagen und statische Dateien gruppiert. Mit Blueprints können Sie Ihre Anwendung in logische Abschnitte unterteilen, die getrennt voneinander entwickelt und getestet werden können.Registrieren von BlueprintsMit der register_blueprint-Methode von Quart registrieren wir jeden Blueprint:for bp in blueprints: app.register_blueprint(bp['blueprint'])Verbindung zur DatenbankWir erstellen eine Hilfsfunktion, um die Verbindung zur Datenbank herzustellen. Diese Funktion versucht mehrmals, eine Verbindung herzustellen und wartet zwischen den Versuchen, um dem Datenbankserver Zeit zu geben, verfügbar zu werden:async def connect_with_retry(connect_func, db_name, max_attempts=6, retry_interval=10): \"\"\" Try to connect to a database with a given connect_func.\"\"\" for attempt in range(max_attempts): try: await connect_func() logger.info(\"Connected successfully to %s !\", db_name) return except Exception as e: logger.info(\"Retrying in %s seconds... (%s/%s)\", retry_interval, attempt+1, max_attempts) await asyncio.sleep(retry_interval) logger.error(\"Max retry attempts reached. Could not connect to %s .\", db_name) raise SystemExit(f\"Could not connect to {db_name}.\")Starten und Beenden der AnwendungWir definieren startup und cleanup Funktionen, die Quart vor bzw. nach dem Start der Anwendung aufruft. In der startup Funktion verbinden wir uns mit den Datenbanken und initialisieren die Managerklassen, während wir in der cleanup Funktion die Verbindungen und Ressourcen aufräumen:@app.before_servingasync def startup(): ...@app.after_servingasync def cleanup(): ...Die Anwendung startenSchließlich erstellen wir die main-Funktion, um die Anwendung zu starten:if __name__ == \"__main__\": quart_config = create_quart_config() config = Config() config.bind = [f\"{quart_config.host}:{quart_config.port}\"] config.certfile = quart_config.certfile config.keyfile = quart_config.keyfile config.protocol = quart_config.protocol config.application_path = \"quartapp:app\" asyncio.run(serve(app, config))Dieser Code startet den Hypercorn-Server und bindet ihn an die von der Konfiguration angegebene Adresse und den Port. Danach serviert Hypercorn die Quart-Anwendung und wartet auf eingehende Anfragen.Das ist alles! Mit diesen wenigen Schritten haben wir eine grundlegende Quart-Anwendung erstellt und mit Hypercorn gehostet. In zukünftigen Blogposts werden wir uns eingehender mit den einzelnen Aspekten dieser Anwendung befassen und uns ansehen, wie wir diese Anwendung weiter anpassen und erweitern können.Schlüsselwörter: #Quart #Hypercorn #WebApp #Python #ASGI #APIs #WebEntwicklung" }, { "title": "Symetrie", "url": "/posts/symetrie/", "categories": "Fotografie", "tags": "foto, bruecke", "date": "2023-03-13 12:00:00 +0100", "snippet": "Inmitten der ruhigen Weiten des Meeres spannt sich eine majestätische Brücke, die sich kerzengerade über den Horizont erstreckt. Es ist ein sonniger Tag in den Niederlanden, und ich stehe genau unt...", "content": "Inmitten der ruhigen Weiten des Meeres spannt sich eine majestätische Brücke, die sich kerzengerade über den Horizont erstreckt. Es ist ein sonniger Tag in den Niederlanden, und ich stehe genau unter der Mitte der Brücke, fasziniert von der perfekten Symmetrie und der Schönheit, die sie verkörpert.Während ich nach oben schaue, kann ich den Blick zwischen den massiven Brückenpfeilern hindurch schweifen lassen. Der Anblick ist atemberaubend: Die Pfeiler, die in den Himmel ragen, scheinen sich im Unendlichen zu verlieren, und der Blick wird von ihnen auf eine magische Reise mitgenommen. Je weiter der Blick schweift, desto kleiner werden die Pfeiler, bis sie schließlich nur noch als winzige Punkte am Horizont zu erkennen sind.In diesem Moment kommt mir eine Anekdote in den Sinn: Es heißt, dass diese Brücke ein Tor zu einer anderen Welt ist. Eine Welt, in der Zeit und Raum ihre Bedeutung verlieren und die Grenzen zwischen Realität und Fantasie verschwimmen. Diejenigen, die den Mut haben, zwischen den Pfeilern hindurchzusehen, können einen flüchtigen Blick auf diese geheimnisvolle Welt erhaschen.Die Legende besagt, dass diejenigen, die diese Brücke überqueren, mit neuer Perspektive und einem Gefühl der Erhabenheit zurückkehren. Die Symmetrie der Brücke symbolisiert die Harmonie und Ordnung, die im Universum existieren, während der Blick durch die Pfeiler hindurch die Möglichkeit offenbart, über die Grenzen der bekannten Welt hinauszublicken.Während ich dort stehe und das Bild der Brücke betrachte, spüre ich die Magie in der Luft. Es ist, als ob diese Brücke ein Portal zu unendlichen Möglichkeiten ist, ein Symbol für die Verbindung zwischen Vergangenheit und Zukunft, zwischen Realität und Imagination.Und so bleibe ich einen Moment länger stehen, unter dieser wunderbaren Brücke, und lasse meine Gedanken in die Ferne schweifen, inspiriert von der Symmetrie und der geheimnisvollen Anekdote, die dieses Bild verkörpert." }, { "title": "Buhnen", "url": "/posts/stille-waechter/", "categories": "Fotografie", "tags": "foto, buhnen", "date": "2023-03-13 12:00:00 +0100", "snippet": "Unter dem endlosen Tanz des Himmels, verankert in der Gezeitenwiege, stehen die Buhnen tapfer auf ihrem Posten. Sie sind keine Helden im Rampenlicht, doch sie verhindern den Strandbuhnen. Ihr Song,...", "content": "Unter dem endlosen Tanz des Himmels, verankert in der Gezeitenwiege, stehen die Buhnen tapfer auf ihrem Posten. Sie sind keine Helden im Rampenlicht, doch sie verhindern den Strandbuhnen. Ihr Song, von der rauen Melodie der Wellen begleitet, erzählt eine Geschichte, so tief und weit wie das Meer selbst. Sie sind die stummen Wächter, die die Bühne für das unendliche Spiel der Natur bereiten. Und im sanften Schimmer der Langzeitbelichtung, tanzen sie einen stillen Walzer mit der Zeit." }, { "title": "Pi-Hole mit Unbound / IPv4 und IPv6", "url": "/posts/pihole-unbound-ipv6/", "categories": "Homelab", "tags": "servers, pihole, dns, docker", "date": "2023-02-10 22:00:00 +0100", "snippet": "Pi-Hole Adblocker in Docker mit Unbound und IPv4 / IPv6Mein SetupBei mir läuft der primäre DNS-Server auf einem Raspberry Pi 4 in einem Docker Container zusammen mit Unbound. Da ich den Pi-Hole auc...", "content": "Pi-Hole Adblocker in Docker mit Unbound und IPv4 / IPv6Mein SetupBei mir läuft der primäre DNS-Server auf einem Raspberry Pi 4 in einem Docker Container zusammen mit Unbound. Da ich den Pi-Hole auch für die lokale Namesauflösung verwende (*.local.cstrube.de) und mir Traeffik hierfür über Let’s Encrypt SSL Zerifikate erstellt brauche ich die Möglichkeit zusammen IPv4 und IPv6 auflösen zu können.1. Statische IP AdresseDem Raspberry Pi mit netplan eine statische IPv4 einrichten:sudo ip achristian@rpi1:~/pihole-unbound$ sudo ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether **:**:**:**:**:** brd ff:ff:ff:ff:ff:ff inet 10.0.50.10/24 brd 10.0.50.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2a02:8071:50d1:2465:e65f:1ff:fe46:efae/64 scope global dynamic mngtmpaddr noprefixroute valid_lft 86345sec preferred_lft 14345sec inet6 fe80::****:***:****:****/64 scope link valid_lft forever preferred_lft foreverunter 2: eth0: sieht man hier den Netzwerknamen. Anschließend erstellen wir eine Datei die wir 01-netcfg.yaml nenen im /etc/netplan Verzeichniss.sudo nano /etc/netplan/01-netcfg.yaml network: ethernets: eth0: addresses: - 10.0.50.10/24 dhcp4: false gateway4: 10.0.50.1 nameservers: addresses: - 1.1.1.1 - 1.0.0.1 search: - workgroup version: 2Hier habe ich DHCP deaktiviert und als DNS Server Cloudflares DNS Server gesetzt (1.1.1.1 und 1.0.0.1)Als nächstes kommen folgende zwei Befehle:sudo netplan generatesudo netplan apply2. Port 53 freimachenDa ich auf dem Raspberry Pi ein Ubuntu 22.04 laufen lasse musste ich als nächstes den Port 53 freiräumen. Dieser wird standardmäßig von resolved belegt.Dafür editiert man die /etc/systemd/resolved.conf Datei:sudo nano /etc/systemd/resolved.confHier wird unter #DNSStubListener=yes die # weggenommen und von =yes auf =no geändert. Siehe Beispiel:[Resolve]# Some examples of DNS servers which may be used for DNS= and FallbackDNS=:# Cloudflare: 1.1.1.1#cloudflare-dns.com 1.0.0.1#cloudflare-dns.com 2606:4700:4700::1111#cloudflare-dns.com 260&gt;# Google: 8.8.8.8#dns.google 8.8.4.4#dns.google 2001:4860:4860::8888#dns.google 2001:4860:4860::8844#dns.go&gt;# Quad9: 9.9.9.9#dns.quad9.net 149.112.112.112#dns.quad9.net 2620:fe::fe#dns.quad9.net 2620:fe::9#dns.quad&gt;#DNS=#FallbackDNS=#Domains=#DNSSEC=no#DNSOverTLS=no#MulticastDNS=no#LLMNR=no#Cache=no-negative#CacheFromLocalhost=noDNSStubListener=no#DNSStubListenerExtra=#ReadEtcHosts=yes#ResolveUnicastSingleLabel=noNach einem Neustart ist Port 53 frei für unseren Docker Container3. Docker installierenAls erstes die Abhängigkeiten installieren:sudo apt-get install apt-transport-https ca-certificates software-properties-common -yAnschließend Docker mit dem offiziellen Script installierencurl -fsSL get.docker.com -o get-docker.sh &amp;&amp; sh get-docker.shJetzt fügen wir noch unseren User der Docker Gruppe hinzu, damit können wir Docker ohne sudo ausführen:sudo usermod -aG docker pi4. Docker für IPv6 vorbereiten:Out of the Box unterstützt Docker kein IPv6, deswegen müssen wir eine /etc/docker/daemon.json Datei erstellen:{ \"ipv6\": true, \"fixed-cidr-v6\": \"fe80::/64\"} Laut offizieller Dokumentation braucht man das nicht, allerdings hatte ich ohne die Datei Probleme damitEinmal Docker Daemon neustarten:sudo systemctl restart dockerAls nächstes brauchen wir iptables damit das Docker Netzwerk IPv6 Traffic empfängt:sudo ip6tables -t nat -A POSTROUTING -s fe80::/64 ! -o docker0 -j MASQUERADEund damit die Änderungen nach einem Neustart nicht verloren gehen gibts das hier:sudo apt install iptables-persistent netfilter-persistentEinfach mit “yes” bestätigen5. Setup Pi-Hole ContainerDa ich ein Fan von Docker Compose bin habe ich hier als erstes eine .env Datei mit den Variablen erstellt:FTLCONF_LOCAL_IPV4=10.0.50.10TZ=Europe/BerlinWEBPASSWORD=sehrsicherespasswortREV_SERVER=trueREV_SERVER_DOMAIN=local.cstrube.deREV_SERVER_TARGET=10.0.10.1REV_SERVER_CIDR=10.0.0.0/16HOSTNAME=pihole-unboundDOMAIN_NAME=pihole-unbound.local.cstrube.dePIHOLE_WEBPORT=80WEBTHEME=default-darkDie Datei liegt im gleichen Verzeichniss wie die docker-compose.yml:version: '3.0'volumes: etc_pihole-unbound:services: pihole: container_name: pihole-unbound image: cbcrowe/pihole-unbound:latest hostname: ${HOSTNAME} domainname: ${DOMAIN_NAME} ports: - 443:443/tcp - 53:53/tcp - 53:53/udp - ${PIHOLE_WEBPORT:-80}:80/tcp environment: - FTLCONF_LOCAL_IPV4=${FTLCONF_LOCAL_IPV4} - TZ=${TZ:-UTC} - WEBPASSWORD=${WEBPASSWORD} - WEBTHEME=${WEBTHEME:-default-dark} - REV_SERVER=${REV_SERVER:-false} - REV_SERVER_TARGET=${REV_SERVER_TARGET} - REV_SERVER_DOMAIN=${REV_SERVER_DOMAIN} - REV_SERVER_CIDR=${REV_SERVER_CIDR} - PIHOLE_DNS_=127.0.0.1#5335 - DNSSEC=\"true\" - DNSMASQ_LISTENING=single - ServerIPv6=\"fe80::***:****:****:****\" volumes: - etc_pihole-unbound:/etc/pihole:rw - ./etc_pihole_dnsmasq-unbound:/etc/dnsmasq.d:rw restart: unless-stopped Wichtig hierbei: ServerIPv6=”…” mit der eigenen ausgelesenen Link-Local Adresse aus “ip a” ausfüllen!/pihole-unbound/docker-compose.ymlDen Container starten wir dann mit:docker compose up -d --force-recreateJetzt muss nur noch im Router der DNS Server auf die IPv4 und IPv6 Adresse des Raspberry Pi geändert werden." }, { "title": "bind9 DNS Server", "url": "/posts/linux-bind-dns-server/", "categories": "Homelab, VMs", "tags": "servers, vm, dns, linux", "date": "2023-02-07 22:00:00 +0100", "snippet": "Ein eigener DNS-Server mit BIND9 bietet folgende Vorteile: Kontrolle über die Namensauflösung: Ein eigener DNS-Server gibt Ihnen die Kontrolle über die Namensauflösung und ermöglicht es Ihnen, Ihr...", "content": "Ein eigener DNS-Server mit BIND9 bietet folgende Vorteile: Kontrolle über die Namensauflösung: Ein eigener DNS-Server gibt Ihnen die Kontrolle über die Namensauflösung und ermöglicht es Ihnen, Ihre eigenen Domänen und IP-Adressen zu verwalten. Verringerung der Latenzzeiten: Ein lokaler DNS-Server kann schnellere Antwortzeiten bieten, da er nicht auf externe DNS-Server angewiesen ist, um Anfragen zu beantworten. Höhere Verfügbarkeit: Ein eigener DNS-Server kann eine höhere Verfügbarkeit gewährleisten, da er nicht auf externe Server angewiesen ist und ein Ausfall dieser Server die Namensauflösung nicht beeinträchtigt. Schutz vor Angriffen: Ein eigener DNS-Server kann dazu beitragen, das Netzwerk vor DNS-basierten Angriffen zu schützen, indem er für eine sichere Übertragung der Daten sorgt. Flexibilität: BIND9 ist ein leistungsstarkes und flexibles System, das anpassbar ist, um spezifische Anforderungen und Bedürfnisse zu erfüllen.Umsetzung:Basis ist ein Ubuntu 22.04 LXC in meinem Proxmox Cluster, vorbereitet mit Docker Engine und Docker ComposeOrdnerstruktur:/bind9/docker-compose.yml/bind9/records//bind9/cache/ docker-compose.ymlversion: \"3\"services: bind9: container_name: dnsserver image: ubuntu/bind9:latest environment: - BIND9_USER=root - TZ=Europe/Berlin ports: - \"53:53/tcp\" - \"53:53/udp\" volumes: - ./config:/etc/bind - ./cache:/var/cache/bind - ./records:/var/lib/bind restart: unless-stopped/bind9/config/named.conf Achtung, hier die eigenen IP Adressen eintragenacl internal { 10.0.10.0/24; 10.0.20.0/24; 10.0.30.0/24; 10.0.70.0/24;};options { forwarders { 1.1.1.1; 1.0.0.1; }; allow-query { internal; };};zone \"local.cstrube.de\" IN { type master; file \"/etc/bind/local.cstrube.zone\";};/bind9/config/local.cstrube.zone local.cstrube.zone$TTL 2d$ORIGIN local.cstrube.de.@ IN SOA ns.local.cstrube.de. info.cstrube.de. ( 2023070222 ; serial 12h ; refresh 15m ; retry 3w ; expire 2h ; minimum ttl ) IN NS ns.local.cstrube.de.ns IN A 10.0.20.11; -- add dns records belowpvecm2 IN A 10.0.10.72pvecm3 IN A 10.0.10.79" }, { "title": "Ubuntu VM mit GPU Passthrough", "url": "/posts/neuer-ubuntu-desktop/", "categories": "Homelab, VMs", "tags": "servers, vm, gpu, ubuntu", "date": "2023-01-29 10:00:00 +0100", "snippet": "Linux Ubuntu 22.04 LTS VM mit GPU NVIDIA Georce 1050Ti UnterstützungEine virtuelle Maschine mit GPU-Beschleunigung auf einem Server bietet folgende Vorteile: Erhöhte Leistung: GPU-Beschleunigu...", "content": "Linux Ubuntu 22.04 LTS VM mit GPU NVIDIA Georce 1050Ti UnterstützungEine virtuelle Maschine mit GPU-Beschleunigung auf einem Server bietet folgende Vorteile: Erhöhte Leistung: GPU-Beschleunigung kann eine signifikante Leistungssteigerung für bestimmte Anwendungen bieten, insbesondere bei grafikintensiven Aufgaben wie Video- und Bildbearbeitung, künstlicher Intelligenz und maschinellem Lernen. Flexibilität: Durch den Einsatz von virtuellen Maschinen kann ein einziger Server mehrere verschiedene Workloads ausführen, die jeweils GPU-Beschleunigung benötigen, wodurch Ressourcen effizienter genutzt werden. Skalierbarkeit: Da virtuelle Maschinen leicht hinzugefügt oder entfernt werden können, kann die GPU-Beschleunigungskapazität des Servers einfach skaliert werden, um den Anforderungen zu entsprechen. Kosteneffizienz: Ein Server mit GPU-Beschleunigung kann kosteneffizienter sein als die Bereitstellung einzelner Workstations mit dedizierten GPUs, insbesondere für kleinere Organisationen oder Unternehmen mit begrenztem Budget. Inhaltsverzeichnis: installiere Proxmox auf Server Setup IOMMU neue Ubuntu VM hinzufügen config Ubuntu VM with PCIE Passthrough1. Proxmox installieren:Am besten der Anleitung hier folgen.2. Setup IOMMU (I/O Memory Management Unit)Hier ist die Anleitung um das Durchreichen vom PCIE Komponenten zu ermöglichen.Das Ergebniss sollte dann so in etwa aussehen:root@pvecm2:~# lspci00:00.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Renoir Root Complex00:00.2 IOMMU: Advanced Micro Devices, Inc. [AMD] Renoir IOMMU00:01.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe Dummy Host Bridge00:01.1 PCI bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe GPP Bridge00:02.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe Dummy Host Bridge00:02.1 PCI bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe GPP Bridge00:02.2 PCI bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe GPP Bridge00:08.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe Dummy Host Bridge00:08.1 PCI bridge: Advanced Micro Devices, Inc. [AMD] Renoir Internal PCIe GPP Bridge to Bus00:14.0 SMBus: Advanced Micro Devices, Inc. [AMD] FCH SMBus Controller (rev 51)00:14.3 ISA bridge: Advanced Micro Devices, Inc. [AMD] FCH LPC Bridge (rev 51)00:18.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166a00:18.1 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166b00:18.2 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166c00:18.3 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166d00:18.4 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166e00:18.5 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166f00:18.6 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 167000:18.7 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 167101:00.0 VGA compatible controller: NVIDIA Corporation GP107 [GeForce GTX 1050 Ti] (rev a1)01:00.1 Audio device: NVIDIA Corporation GP107GL High Definition Audio Controller (rev a1)02:00.0 USB controller: Advanced Micro Devices, Inc. [AMD] Device 43ee02:00.1 SATA controller: Advanced Micro Devices, Inc. [AMD] Device 43eb02:00.2 PCI bridge: Advanced Micro Devices, Inc. [AMD] Device 43e903:00.0 PCI bridge: Advanced Micro Devices, Inc. [AMD] Device 43ea04:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8125 2.5GbE Controller (rev 04)05:00.0 Non-Volatile memory controller: Sandisk Corp WD Black SN850 (rev 01)06:00.0 Non-Essential Instrumentation [1300]: Advanced Micro Devices, Inc. [AMD] Zeppelin/Raven/Raven2 PCIe Dummy Function (rev c9)06:00.1 Audio device: Advanced Micro Devices, Inc. [AMD/ATI] Device 163706:00.2 Encryption controller: Advanced Micro Devices, Inc. [AMD] Family 17h (Models 10h-1fh) Platform Security Processor06:00.3 USB controller: Advanced Micro Devices, Inc. [AMD] Renoir USB 3.106:00.4 USB controller: Advanced Micro Devices, Inc. [AMD] Renoir USB 3.106:00.6 Audio device: Advanced Micro Devices, Inc. [AMD] Family 17h (Models 10h-1fh) HD Audio Controller3. neue VM erstellen: Öffne die Proxmox-Web-Oberfläche Klick auf den Reiter “VMs” Wähle “Create VM” im Dropdown-Menü oder klicke auf den grünen “Create VM”-Button Wähle den gewünschten Betriebssystemtyp aus und gib die grundlegenden Einstellungen wie RAM und CPU-Kerne an (Maschinentyp Q35, UEFI) Wähle ein Storage-Ziel für die neue VM aus Lade ein Betriebssystem-Image hoch oder wähle ein bereits vorhandenes Image Klicke auf “Create”4. VM für Passthrough konfigurieren:Achtung: Die VM ist nach dem hinzufügen der GPU nicht mehr über die Console erreichbar. Vorher sicherstellen das man mit VNC oder RDP auf die Maschine zugreifen kann. Ubuntu 22.04 hat z.B. RDP mit an Bord. Gehe zur Proxmox-Web-Oberfläche und öffne den virtuellen Maschinen-Editor Wähle im Reiter “Hardware” den PCIE-Gerätetyp aus und klicke auf “PCI passthrough” Markiere das zu übertragende Gerät und klicke auf “Add” Setze die Häckchen bei: (X)All Funktions, (X) Primary GPU, (X) ROM BAR, (X) PCI-Express Starte die virtuelle Maschine neu Überprüfe im Betriebssystem der virtuellen Maschine, ob das Gerät erkannt wurde5. NVIDIA X-Window Treiber installieren" }, { "title": "pterodactyl Gameserver", "url": "/posts/gameserver/", "categories": "Gameserver", "tags": "servers, eggs", "date": "2023-01-27 10:00:00 +0100", "snippet": "https://gameserver.cstrube.deDer Gameserver wird ab sofort über eine Cloudflare Tunnel von aussen erreicht.Der Tunnel Daemon selbst läuft auf dem Gameserver als Docker Container.", "content": "https://gameserver.cstrube.deDer Gameserver wird ab sofort über eine Cloudflare Tunnel von aussen erreicht.Der Tunnel Daemon selbst läuft auf dem Gameserver als Docker Container." }, { "title": "neue QnA Webseite", "url": "/posts/answer/", "categories": "QnA", "tags": "servers, answer", "date": "2023-01-08 10:00:00 +0100", "snippet": "https://qna.cstrube.deIch hatte mir überlegt eine geschlossene Community Seite zu erstellen. Diese wird über Freunde und Kollegen geteilt um eine Lösungsorientierte Community zusammen zu bekommen.D...", "content": "https://qna.cstrube.deIch hatte mir überlegt eine geschlossene Community Seite zu erstellen. Diese wird über Freunde und Kollegen geteilt um eine Lösungsorientierte Community zusammen zu bekommen.Das Grundgerüst der Seite ist ein opensource Projekt vonhttps://hub.docker.com/r/answerdev/answerDiese läuft hier auf dem Rapsberry Pi 4/1, wird über einen CF Tunnel nach außen angebunden." }, { "title": "openai ChatGPT", "url": "/posts/ChatGPT/", "categories": "AI", "tags": "servers, dell, hp, supermicro", "date": "2022-12-18 01:00:01 +0100", "snippet": "ChatGPT", "content": "ChatGPT" }, { "title": "Konfiguration von Jekyll als systemd-Dienst auf Ubuntu", "url": "/posts/linux-jekyll-systemd-ubuntu/", "categories": "Homelab", "tags": "servers, pi, mdns, nginx, webserver, raspberry, ubuntu, systemd, jekyll", "date": "2022-08-01 14:00:00 +0200", "snippet": "EinleitungDieser Beitrag soll Ihnen zeigen, wie Sie Jekyll als einen systemd-Dienst auf einem Ubuntu-Server einrichten können. Dies ist besonders nützlich, wenn Sie eine Jekyll-Website hosten und s...", "content": "EinleitungDieser Beitrag soll Ihnen zeigen, wie Sie Jekyll als einen systemd-Dienst auf einem Ubuntu-Server einrichten können. Dies ist besonders nützlich, wenn Sie eine Jekyll-Website hosten und sicherstellen möchten, dass sie immer läuft, auch nach einem Neustart des Servers.Voraussetzungen Einen Ubuntu-Server, VM oder LXC-Container Ruby und Bundler installiert Anleitung Ein Jekyll-Projekt z.B. nach dieser Vorlage: jekyll-docs-site @timothystewart6KonfigurationSchritt 1: Zunächst sollten wir eine Umgebungsvariable für Jekyll festlegen. Dafür öffnen Sie die .bashrc oder .bash_profile Datei in Ihrem Home-Verzeichnis und fügen die folgende Zeile hinzu:export JEKYLL_ENV=productionSchritt 2: Führen Sie source ~/.bashrc aus, um die neue Konfiguration zu laden.Schritt 3: Installieren Sie alle notwendigen Gems mit Bundler. In Ihrem Jekyll-Projektverzeichnis führen Sie die folgenden Befehle aus:bundle config set --local path 'vendor/bundle'bundle installDies sorgt dafür, dass alle Gems in einem Unterordner (vendor/bundle) Ihres Projekts installiert werden.Schritt 4: Erstellen Sie nun eine systemd-Service-Datei für Jekyll. Erstellen Sie die Datei /etc/systemd/system/jekyll.service und fügen Sie den folgenden Inhalt ein:[Unit]Description=Jekyll BuildAfter=network.target[Service]Type=simpleUser=christianWorkingDirectory=/home/IHR_BENUTZERNAME/GITHUB_USERNAME.github.ioEnvironment=\"BUNDLE_PATH=/home/IHR_BENUTZERNAME/GITHUB_USERNAME.github.io/vendor/bundle\"Environment=\"GEM_HOME=/home/IHR_BENUTZERNAME/.gem/ruby/3.0.0\"Environment=\"JEKYLL_ENV=production\"ExecStart=/bin/bash -lc '/home/IHR_BENUTERNAME/gems/bin/bundle exec /home/IHR_BENUTERNAME/gems/bin/jekyll build -w'CPUQuota=20%[Install]WantedBy=multi-user.targetErsetzen Sie IHR_BENUTZERNAME durch Ihren Benutzernamen und PFAD/ZU/IHREM/JEKYLL_PROJEKT durch den tatsächlichen Pfad zu Ihrem Jekyll-Projekt.Ersetzen Sie GITHUB_USERNAME durch Ihren GitHub-Benutzernamen. CPUQuota=20%, der Dienst darf maximal 20% der Verfügbaren CPU Zeit verbrauchen, dies ist sinnvoll, wenn Sie den Dienst auf einem Raspberry Pi betreiben wollen, damit der Dienst nicht die ganze CPU Zeit verbraucht und andere Dienste nicht mehr reagieren können.Schritt 5: Aktivieren und starten Sie den Dienst mitsudo systemctl enable jekyll.servicesudo systemctl start jekyll.serviceFehlerbehebungSollten Sie auf Probleme stoßen, überprüfen Sie, ob alle Pfade korrekt sind und ob der angegebene Benutzer die richtigen Berechtigungen hat. Bei spezifischen Gem-Problemen kann es hilfreich sein, diese manuell zu installieren oder verschiedene Versionen zu testen.AbschlussJetzt sollten Sie eine laufende Jekyll-Instanz auf Ihrem Server haben, die nach einem Systemneustart automatisch wieder gestartet wird. Sie können den Status des Dienstes jederzeit mit systemctl status jekyll.service überprüfen.Um den Dienst du überwachen eignet sich auch wunderbar:sudo journalctl -u jekyll.service -xe" }, { "title": "mDNS mit VLAN nutzen (zb. Raspberry Pi)", "url": "/posts/mDNS-refelector-with-vlan-avahi/", "categories": "Homelab", "tags": "servers, pi, mdns, vlan, avahi, raspberry", "date": "2022-07-31 17:00:00 +0200", "snippet": " Apple Airprint geht nicht Zugriff auf Drucker und Apple Airplay nicht möglich Omada SDN kann (noch) kein mDNS RefelectorSetupEin Raspberry Pi im “Client” Netz mit getaggten WLAN und IOT LAN VLA...", "content": " Apple Airprint geht nicht Zugriff auf Drucker und Apple Airplay nicht möglich Omada SDN kann (noch) kein mDNS RefelectorSetupEin Raspberry Pi im “Client” Netz mit getaggten WLAN und IOT LAN VLAN (bei mit 70,72 und 40) mit eth0 verbunden.Avahi Daemin installieren:sudo apt update &amp;&amp; sudo apt upgradesudo apt install avahi-daemonDaemon starten:/etc/init.d/avahi-daemon startoder:sudo systemctl start avahi-daemonanschließend für Automatischen Start:sudo systemctl enable avahi-daemonAvahi-Daemon konfigurierensudo nano /etc/avahi/avahi-daemon.conf[server]..use-ipv4=yesuse-ipv6=yes..[wide-area]enable-wide-area=yes[reflector]enable-reflector=yes#reflect-ipv=no#reflect-filters=_airplay._tcp.local,_raop._tcp.localVLAN im Network Interfacesudo nano /etc/network/interfaces.d/vlans# WLANauto eth0.70iface eth0.70 inet manual vlan-raw-device eth0# IOT WLAN/LANauto eth0.72iface eth0.72 inet manual vlan-raw-device eth0# Client VLANauto eth0.40iface eth0.40 inet manual vlan-raw-device eth0zum Schluss noch:sudo systemctl restart avahi.daemon$bash: ip a...eth0.70@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b8:27:eb:0c:2e:ef brd ff:ff:ff:ff:ff:ff inet 10.0.70.155/24 brd 10.0.70.255 scope global dynamic noprefixroute eth0.70 valid_lft 5364sec preferred_lft 4464sec inet6 fd47:776d:f64c:49d0:caba:69cd:3878:66ce/64 scope global deprecated dynamic mngtmpaddr noprefixroute valid_lft 999sec preferred_lft 0sec inet6 2a02:908:4c24:62c7:597c:3ee7:71ab:c300/64 scope global dynamic mngtmpaddr noprefixroute valid_lft 86003sec preferred_lft 14003sec inet6 fe80::ba27:ebff:fe0c:2eef/64 scope link valid_lft forever preferred_lft forever5: eth0.30@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b8:27:eb:0c:2e:ef brd ff:ff:ff:ff:ff:ff inet 10.0.30.100/24 brd 10.0.30.255 scope global dynamic noprefixroute eth0.30 valid_lft 5359sec preferred_lft 4459sec inet6 2a02:908:4c24:62c3:2c4:3aab:6b43:4cc9/64 scope global dynamic mngtmpaddr noprefixroute valid_lft 86328sec preferred_lft 14328sec inet6 fe80::6b73:b6bc:98bc:33a1/64 scope link valid_lft forever preferred_lft forever inet6 fe80::ba27:ebff:fe0c:2eef/64 scope link valid_lft forever preferred_lft forever6: eth0.20@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b8:27:eb:0c:2e:ef brd ff:ff:ff:ff:ff:ff inet 10.0.20.101/24 brd 10.0.20.255 scope global dynamic noprefixroute eth0.20 valid_lft 5365sec preferred_lft 4465sec inet6 2a02:908:4c24:62c2:1247:382:5e75:6c24/64 scope global dynamic mngtmpaddr noprefixroute valid_lft 86081sec preferred_lft 14081sec inet6 fe80::3ace:6f97:dffc:2acc/64 scope link valid_lft forever preferred_lft forever7: eth0.72@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b8:27:eb:0c:2e:ef brd ff:ff:ff:ff:ff:ff inet 10.0.72.3/24 brd 10.0.72.255 scope global dynamic noprefixroute eth0.72 valid_lft 5366sec preferred_lft 4466sec inet6 2a02:908:4c24:62c8:a73a:6449:c13d:f9dd/64 scope global dynamic mngtmpaddr noprefixroute valid_lft 86087sec preferred_lft 14087sec inet6 fe80::68c9:c9c4:da3b:aa23/64 scope link valid_lft forever preferred_lft forever...ipv4 und ipv6FazitAirprint und Airplay funktionieren über getrennte VLAN hinweg." }, { "title": "sechster Tag auf Gran Canaria / Der Nord-Westen", "url": "/posts/Sechster-Tag-auf-Gran-Canaria/", "categories": "Reisen, Gran Canaria", "tags": "strand, wandern, agate", "date": "2022-07-08 21:00:00 +0200", "snippet": "Da für diesen Tag eine Hitzewarnung für unsere Region ausgesprochen wurde, planten wir einen Ausflug zu einem Strand im Nordwesten der Insel, genauer gesagt, in der Nähe von Agaete. Dieser Strand ...", "content": "Da für diesen Tag eine Hitzewarnung für unsere Region ausgesprochen wurde, planten wir einen Ausflug zu einem Strand im Nordwesten der Insel, genauer gesagt, in der Nähe von Agaete. Dieser Strand ist etwas versteckt und nur zu Fuß erreichbar. Leider hüllte sich die Gegend morgens noch in eine dicke Wolkendecke, sodass wir uns gegen den Abstieg entschieden und stattdessen ein Stück weiter in Richtung Süden fuhren.Playa del GuayedraUnser neues Ziel war der PLaya del Guayedra südlich von Agaete. Auch hier musste man ein kleines Stück zu Fuß die Klippen hinuntersteigen, doch dank festem Schuhwerk stellte das kein Problem dar. Der nächste fantastische Aspekt war: Wir hatten den Strand die nächsten zweieinhalb Stunden ganz für uns alleine!Es gibt dort einen weiteren Strandabschnitt mit kleineren Steinen, der sich besser zum Baden eignet. Doch der eigentliche Star des Tages waren nicht die fabelhaften Strände, sondern unsere neuen Freunde: Eine Schar mutiger Eidechsen, die nach und nach aus den Felsspalten krochen. Sie schienen von uns FKK-liebenden, sonnenbadenden Menschen fasziniert zu sein und uns neugierig zu beobachten. Es war, als hätten sie ihre eigene Version von ‘David Attenborough’s Planet Earth’ gestartet, nur dass wir die Hauptattraktion waren! Ein unvergesslicher Tag in Gesellschaft unserer reptilischen Zuschauer, die sicherlich auch ihre eigene Sonnentag-Geschichte zu erzählen haben." }, { "title": "Fünfter Tag auf Gran Canaria / Nationalpark Tamadaba", "url": "/posts/Fuenfter-Tag-auf-Gran-Canaria/", "categories": "Reisen, Gran Canaria", "tags": "tamadaba, wandern, tambada", "date": "2022-07-07 21:00:00 +0200", "snippet": "Inmitten des quirligen Treibens und der wunderschönen Strände von Gran Canaria gibt es einen Ort, an dem man der Hektik des Alltags entfliehen und sich in der Stille der Natur verlieren kann: der N...", "content": "Inmitten des quirligen Treibens und der wunderschönen Strände von Gran Canaria gibt es einen Ort, an dem man der Hektik des Alltags entfliehen und sich in der Stille der Natur verlieren kann: der Nationalpark Tamadaba. Gelegen im nordwestlichen Teil der Insel, ist dieser Park ein wahres Juwel und bietet einen atemberaubenden Ausblick auf die kanarischen Kiefernwälder.Unsere Wanderung begann auf einem abgelegenen Parkplatz mitten im Nationalpark. Zu unserer Überraschung waren wir fast die einzigen Besucher an diesem Tag - nur zwei weitere Fahrzeuge waren in Sichtweite. Dies versprach eine ruhige und ungestörte Wanderung, fernab der üblichen Touristenpfade.Mit jedem Schritt, den wir in die westliche Richtung machten, wurden wir tiefer in den duftenden Nadelwald hineingezogen. Der süße, erdige Geruch der Kiefern vermischte sich mit der frischen Bergluft und schuf ein aromatisches Bouquet, das jeden Atemzug zu einem Genuss machte.Bald darauf bot sich uns ein atemberaubender Anblick: am Horizont tauchte die Silhouette der Insel Teneriffa auf. Der Anblick dieses majestätischen Nachbarn in der Ferne war eine Erinnerung daran, wie weit wir von der Hektik der Stadt entfernt waren.Nachdem wir den Gipfel umrundet hatten, begann der anspruchsvollste Teil unserer Wanderung: der steile Aufstieg. Doch die Anstrengung lohnte sich. Mit jedem gewonnenen Höhenmeter offenbarte sich uns eine immer beeindruckendere Aussicht auf die umliegende Landschaft.Oben angekommen, wurden wir mit einem Panoramablick belohnt, der uns den Atem raubte. Die grünen Kiefernwälder erstreckten sich soweit das Auge reichte, durchzogen von malerischen Wanderwegen und flankiert von den blauen Weiten des Atlantiks. In diesen Momenten der Stille und Schönheit fühlten wir uns wirklich mit der Natur verbunden.Unsere Wanderung im Nationalpark Tamadaba war mehr als nur ein Ausflug in die Natur. Es war eine Chance, den Alltag hinter uns zu lassen und in die beeindruckende Schönheit der kanarischen Landschaft einzutauchen. Dieser Ort wird uns immer in Erinnerung bleiben - als ein Ort der Ruhe und Erneuerung inmitten der lebhaften Szenerie von Gran Canaria." }, { "title": "Vierter Tag auf Gran Canaria / Caldera de Bandama", "url": "/posts/Vierter-Tag-auf-Gran-Canaria/", "categories": "Reisen, Gran Canaria", "tags": "caldera, wandern, bandama", "date": "2022-07-06 21:00:00 +0200", "snippet": "Der Krater ist nur wenige Autominuten von unserer Unterkunft entfernt und entstand durch eine Eruption. Da ist einfach ein riesen Loch in der hügeligen Landschaft mit 1000m Meter Durchmesser und 24...", "content": "Der Krater ist nur wenige Autominuten von unserer Unterkunft entfernt und entstand durch eine Eruption. Da ist einfach ein riesen Loch in der hügeligen Landschaft mit 1000m Meter Durchmesser und 240m Tiefe. Caldera de BandamaRundwegAuf dem Rand des Kraters gibt einen Rundweg für den wir gut 90 Minuten gebraucht haben. Teilweise geht es sehr steil auf und ab, der Untergrund ist loses Geröll. Rundweg auf dem Gradvon der anderen SeiteAusblickVom Rand aus hat man einen schönen Ausblick auf die Ostküste Gran CanariasValle de Jinamar" }, { "title": "Dritter Tag auf Gran Canaria / Der Wolkenfels", "url": "/posts/Dritter-Tag-auf-Gran-Canaria/", "categories": "Reisen, Gran Canaria", "tags": "roquenublo, wandern", "date": "2022-07-05 21:00:00 +0200", "snippet": "Am Morgen unseres Ausflugs war das Wetter alles andere als vielversprechend. Regen und niedrige 15°C schienen nicht die idealen Bedingungen für eine Wanderung zu sein. Dennoch ließen wir uns nicht ...", "content": "Am Morgen unseres Ausflugs war das Wetter alles andere als vielversprechend. Regen und niedrige 15°C schienen nicht die idealen Bedingungen für eine Wanderung zu sein. Dennoch ließen wir uns nicht entmutigen und machten uns gegen 9:30 Uhr auf den Weg nach Tejeda, mit dem Ziel, den beeindruckenden “Wolkenfelsen”, den Roque Nublo, zu erkunden.Die Fahrt war ein Erlebnis für sich. Die kurvigen Straßen, die uns durch atemberaubende Landschaften führten, waren ein fotografischer Genuss. Wir konnten nicht widerstehen, einige Stopps einzulegen, um die Panoramaaussichten einzufangen.Als wir die Wolkendecke durchstießen, offenbarte sich uns ein ganz anderer Anblick. Die grauen Wolken waren einem klaren blauen Himmel gewichen, und die Temperatur war angenehm gestiegen.Roque NubloAm Parkplatz La Goleta begann unsere Wanderung zum Fels. Mit steigender Höhe wurde es wärmer - die Temperatur erreichte schließlich 33°C. Der Weg zum Gipfel betrug etwa 1,5 km und war ein stetiger Aufstieg. angekommen startete die kleine Wanderung hinauf zum Fels. Dabei stieg die Temperatur auf warme 33°C an. Es sind von dort aus ca. 1,5km bergauf.Oben angekommen, konnten wir den beeindruckenden Ausblick genießen, während die Sonne unsere Haut bräunte. Nach etwa 30 Minuten am Gipfel machten wir uns auf den Rückweg zum Parkplatz.MittagessenFür unser Mittagessen kehrten wir in das bei Touristen beliebte Restaurant El Mirado de Tunte ein, das sich unweit des Roque Nublo befindet. Das Essen war in Ordnung, obwohl wir just in dem Moment ankamen, als ein Bus voller Touristen eintraf. Die beiden Kellner waren sichtlich bemüht, alle Gäste gleichzeitig zu versorgen.Nach diesem stärkenden Halt fuhren wir wieder durch die Kurven und Schluchten zurück zu unserer Unterkunft, mit der Gewissheit, einen weiteren unvergesslichen Tag auf Gran Canaria verbracht zu haben." }, { "title": "Zweiter Tag auf Gran Canaria / Strand und Sonne pur", "url": "/posts/Zweiter-Tag-auf-Gran-Canaria/", "categories": "Reisen, Gran Canaria", "tags": "playadelingles", "date": "2022-07-04 21:00:00 +0200", "snippet": "Nach dem Einkauf eininger Lebensmittel frühstückten wir gemütlich und bereiteten uns für den Tag vor. Da der Wetterbericht suggerierte es würde am Ende der Woche heißer werden planten wir den Tag a...", "content": "Nach dem Einkauf eininger Lebensmittel frühstückten wir gemütlich und bereiteten uns für den Tag vor. Da der Wetterbericht suggerierte es würde am Ende der Woche heißer werden planten wir den Tag am Playa del Ingles und den dortigen Dünen zu verbringen.Playa Del InglesDünenNach einem langen Strand Spaziergang gönnten wir uns eine Abkühlung im Meer (das Badewannentemperatur hatte).MeerMittagessenDa in unserem Reiseführer das Lokal Rias Bajas sehr gelobt wurde liefen wir vom Strand die 2 km in die Innenstadt von Playa del Ingles. Hier bemerkten wir dann auch die 30°C Lufttemperatur die ohne den Wind vom Meer recht warm wirkten.Das Lokal ist von gehobener Klasse, dies führte dazu das wir uns in T-Shirt und Kurzer Hose etwas unterdressed fühlten. Dennoch wurden wir freundlich empfangen und auch gut bewirtet, das Essen (wir hatten gebacktenen Seehecht mit Salat) war vorzüglich.Rückweg zum MeerNach dem Essen liefen wir zurück zum Parkplatz auf dem unser Mietwagen stand und nahmen diesen diesmal mit auf den Parkplatz direkt am Strand. Anschließend gab es noch ein Schwimmeinlage im Meer mit anschließendem Rösten auf dem Handtuch. Zum Abschluß gönnten wir uns an der Uferpromenade bei einer der vielen Touristenbuden einen Smoothie und verließen Playa del Ingles gegen 17:00 Uhr.AbendDen Abend verbrachten wir in unserer Suite da hier der Wind auffrischte und ein draussen sitzen ungemütlich machte. Nach all der Sonne an diesem Tag war das aber nicht weiter schlimm." }, { "title": "Erste Eindrücke von Gran Canaria", "url": "/posts/Erster-Tag-auf-Gran-Canaria/", "categories": "Reisen, Gran Canaria", "tags": "condor, fra", "date": "2022-07-03 21:00:00 +0200", "snippet": "Flughafen Übersichtliches Gate Technisches Waschbecken von Dyson im KloMietwagenWir hatten über Cicars einen FIAT 500 reserviert. Als uns der freundliche Herr der Autovermietung mit unseren Koffe...", "content": "Flughafen Übersichtliches Gate Technisches Waschbecken von Dyson im KloMietwagenWir hatten über Cicars einen FIAT 500 reserviert. Als uns der freundliche Herr der Autovermietung mit unseren Koffern sah schlug er uns sofort ein kostenloses Upgrade auf einen OPEL Corsa vor. Nach kurzer Eingewöhungszeit erinnerte ich mich auch an den Sinn des Kupplungspedals.OPEL Corsa von CiCars Gran CanariaGran CanariaNoch bevor wir unsere Unterkunft aufsuchten wollten wir die Insel auf eigene Faust mit dem Mietwagen erkunden. Also fuhren wir vom Flughafen aus zuerst einmal Richtung Norden in der die Hauptstadt von Gran Canaria liegt.Von Las Palmas aus versuchten wir weiter Westlich zu Fahren, allerdings gefiel uns dort die Umgebung nicht so gut. Somit kehrten wir nach Las Palmas zurück und verbrachten den Mittag unter mit Wolken bedecktem Himmel dort am Strand.Wegweiser an der UferpromenadeStrand von Las PalmasMittagessenLangsam fühlte es sich etwas flau im Magen an, etwas Nahrung musste her.Wir fanden eine nette kleine LocationUnterkunftWir haben uns nach langem Suchen für Albor Suites entschieden. Und dies haben wir nicht bereut. Zu dem Anwesen gehört ein Geschmeinschaftspool. Da wir im Moment anscheinend die einzigen Gäste hier sind ist es fast ein Privat Pool. Die Gastgeber sind sehr herzlich und kümmern sich gut um uns.Sonntag AbendEin kleiner Spaziergang rund um unsere Unterkunft rundete den ersten Tag ab.Santa BrígidaEl VinculoEigener ReiseführerAls letzten Schritt planten wir unseren nächsten Tag über den Reiseführer der in Apple Maps vorhanden ist." }, { "title": "Abflug nach Gran Canaria am Flughafen Frankfurt am Main", "url": "/posts/Abflug-in-FFM/", "categories": "Reisen, Fliegen", "tags": "condor, fra", "date": "2022-07-03 08:25:00 +0200", "snippet": "Die Koffer sind gepackt, hier eine kleine Auswahl was alles mit dabei ist: Kameras MacBook Air KlamottenGate A am Flughafen Frankfurt am Main 5:00 UhrVorabend Koffer Check-InAufgrund der akutell...", "content": "Die Koffer sind gepackt, hier eine kleine Auswahl was alles mit dabei ist: Kameras MacBook Air KlamottenGate A am Flughafen Frankfurt am Main 5:00 UhrVorabend Koffer Check-InAufgrund der akutellen Situation am FRA hat man uns zum Vorabend Check-In der Koffer geraten. Also haben wir uns am Samstag um 17:00 Uhr auf den Weg zum Flughafen gemacht. Mit dem Auto in die Innenstadt FFM, von dort aus mit der S8 weiter zum Airport. Da an diesem Abend Cold-Play in der Commerzbank Arena auftrat war die S-Bahn gerammelt voll.Am Flughafen Halle C angekommen suchten wir den Schalter 778 der Condor. Eine lange Schlange Reisender versperrte uns die Sicht. Nach kurzer Zeit merkten wir dann das dies unsere Schlange zum Schalter 778 ist…die Wartezeit betrug 1:30 Stunden. Auf dem Heimweg gönnten wir uns noch einen überteuerten Kaffee bei diesem US Franchise mit der Krone und waren gegen 21:00 Uhr wieder zuhause.SicherheitskontrolleUm 3:00 Uhr nachts machten wir uns erneut auf den Weg zum FRA, diesmal mit dem Auto um den von uns vorher reservierten Stellplatz im P2/P3 aufzusuchen.4:00 Uhr: Da wir uns selbst online eingecheckt haben und die Koffer bereits am Flughafen waren durften wir direkt zur Sicherheits Kontrolle. Als wir eine lange Schlange Reisender sahen ahnten wir bereits was auf uns zu kommt. Auch hier wieder 1:30 Stunden Wartezeit. Als wir endlich an der Reihe waren und den Ganzkörperscan bestanden hatten (Wurde gefragt warum ich so schwitze, ob ich gerannt sei?. Diese Frage zog der Fragende sofort wieder zurück nachdem er einen Blick in die Schlange riskiert hatte.)Auch der Kamera Rucksack erweckte die Aufmerksamkeit der Prüfenden und wurde mit einer Sonde im Inneren zum Glück nicht nach Katzenhaaren untersucht.5:30 Uhr: Endlich am Gate A23 angekommen warteten wir auf das Boarding das um 5:40 Uhr beginnen sollte. Gegen 6:00 Uhr kam die Durchsage für einen Gate Change so daß alle Reisenden hektisch 50 Meter zum anderen Gate hasteten. Das Boarding begann dann wirklich um kurz vor 7:00 Uhr.Koffertracking mit AirTagsWir haben in unsere Koffer AirTags gepackt…und das hat erstaunlich gut funktioniert.Nicht nur da wir wussten ob unsere Koffer auch im richtigen Flugzeug sind, sondern auch bei der Ankunft am Ziel Flughafen konnten wir einfach warten bis sich die AirTags in der Nähe befanden um unsere Koffer sehr schnell auf dem Kofferband zu entdecken.Der FlugDer Flug selbst startete mit gut einer Stunde Verspätung (weil noch nicht alle Koffer an Bord waren). Da wir XL Sitze gebucht hatten saßen wir direkt neben der vorderen Eingangtür und neben der Toilette. Man glaubt gar nicht wie oft diese auf einem 4:30 Stunden Flug besucht wird. Ca. 30 Minuten nach dem Start gab es den “Snack” von Condor und ein Glas Wasser. Die Toilette suchten wir aus Gründen nicht mehr auf." }, { "title": "Entdecken Sie die Welt von Christian Strube: Reisen, Technologie und künstliche Intelligenz", "url": "/posts/Willkommen/", "categories": "Technologie", "tags": "gpt-4, künstliche, intelligenz, technologie, lifestyle", "date": "2022-01-01 01:00:00 +0100", "snippet": "Willkommen auf dem Blog von Christian Strube, einem Ort, an dem Sie sich auf eine faszinierende Reise durch verschiedene Bereiche des Lebens begeben können. Hier finden Sie spannende Beiträge zu de...", "content": "Willkommen auf dem Blog von Christian Strube, einem Ort, an dem Sie sich auf eine faszinierende Reise durch verschiedene Bereiche des Lebens begeben können. Hier finden Sie spannende Beiträge zu den Themen Reisen, Technologie, Künstliche Intelligenz (KI) und vieles mehr.Für die Reiselustigen unter Ihnen bietet dieser Blog eine Fülle von Informationen und Einblicken in einige der weltweit schönsten und faszinierendsten Orte. Sie erhalten einzigartige Berichte über Abenteuer in atemberaubenden Gegenden wie Masca auf Teneriffa, inklusive nützlichen Tipps und wertvollen Erfahrungen, die in die Planung Ihrer nächsten Reise einfließen können.Liebhaber von Technologie werden hier ebenso fündig. Tauchen Sie ein in die Welt der neuesten Technologie-Trends, von aufstrebenden Technologien über Hardware bis hin zu Softwarelösungen. Erkunden Sie die Vielfalt der Möglichkeiten, die uns die fortschrittliche Technologie bietet, und entdecken Sie, wie sie unseren Alltag beeinflusst und verbessert.Ein weiterer Schwerpunkt dieses Blogs ist das spannende und ständig weiterentwickelnde Feld der Künstlichen Intelligenz (KI). Mit tiefergehenden Einblicken in die Mechanismen und Potenziale der KI können Sie verstehen, wie diese Technologie unsere Welt revolutioniert. Von der Verbesserung von Geschäftsprozessen über den Einsatz in der Medizin bis hin zu ihrem Einfluss auf unser tägliches Leben - die KI ist eine faszinierende Reise wert.Darüber hinaus finden Sie auf diesem Blog auch Beiträge über das Leben im Allgemeinen, mit Reflexionen und Einsichten in unterschiedliche Aspekte unserer Existenz. Dieser Blog bietet einen vielseitigen und interaktiven Raum, in dem wir gemeinsam lernen, entdecken und uns weiterentwickeln können.Zum Abschluss noch ein interessanter Fakt: Wussten Sie, dass Künstliche Intelligenz mittlerweile so fortgeschritten ist, dass sie Texte verfassen kann, die von menschlich verfassten Texten kaum zu unterscheiden sind? Ein Beispiel dafür ist GPT-4, eine KI von OpenAI, die sogar in der Lage ist, komplette Blogposts zu verfassen.Schauen Sie regelmäßig vorbei, um keine neuen Beiträge zu verpassen und sich kontinuierlich über spannende Themen zu informieren. Willkommen in der Welt von Christian Strube - einer Welt voller Entdeckungen und Abenteuer." } ]
